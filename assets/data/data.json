{
 "items": [
  {
    "id": "QfYHisYpTb-D1qmbXY5AFQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "weworkremotely.com-programming",
    "url": "https://weworkremotely.com/jobs/4651-spark-elasticsearch-data-engineer",
    "title": "Streamhub: Spark / ElasticSearch Data Engineer (Contractor, UK only)",
    "content": "<div>  <div>    <h2>Spark / ElasticSearch Data Engineer (Contractor, UK only)</h2>      <h4>Posted Apr 26</h4>    <h3>      <span>Streamhub</span><br>      <span>Headquarters: London, UK</span><br>        <a href=&quot;http://www.streamhub.co.uk&quot; rel=&quot;nofollow&quot;>www.streamhub.co.uk</a>    </h3>  </div>    </div>  <div>    <div>Streamhub is a video analytics startup providing Data-As-A-Service to premium media businesses around the world. In short, we are a Google Analytics for VOD. If you have a knack for distributed systems and discovering value out of the mountains of viewing data we get from our audiences, then this is for you.</div><div><br></div><div>We are looking for an experienced senior big data engineer (contractor) with expertise in Spark and ElasticSearch. The job will be mostly remote but we ask you to come into our London office at the start and from time to time during the 2-3 month project.&nbsp;</div><div><br></div><div>You will play a key role in our big data engineering team, which is a 100% hands-on.In line with Streamhub’s ambitions, we are challenging to derive real-world value and structure from the vast range of “video content” and “audience” data all around us.</div><div><br></div><div>You can expect hard but rewarding work that brings your skills into the world. We have a lean, pragmatic and open culture that values communication and a focus on iterative development.&nbsp;</div><div><br></div><div><span>The Challenges:</span><br></div><ul><li>Design, implement and support new features for Elasticsearch and Cassandra</li><li>Build and support features for Spark, Hadoop.</li><li>Optimise the integration of new data sources, addition of business logic that provide further layers and dimensions to the data analysis</li></ul><div><br></div><div><span>We would need you to:</span></div><div><br></div><ul><li>Be deeply profficient and have 4+ years in object oriented development, preferably Java or Scala.</li><li>Expert in Elasticsearch, including its operational aspects.</li><li>Have 2+ years of hands-on experience with Spark, Cassandra and the Hadoop ecosystem technologies</li><li>Experience with AWS big data stack</li><li>Must be a great team player with proven reading, writing and speaking of English</li><li>Experience with DSPs, SSPs, DMPs are a big plus</li><li>Machine learning and predictive algorithms a further plus</li></ul><div><br></div><div>Our technology stack that will keep you busy</div><ol><li><span>Scala/Java</span></li><li><span>Apache Spark or Hadoop stack</span></li><li><span>AWS big data stack</span></li><li><span>Cassandra</span></li><li>Apache Nifi</li><li><span>Elasticsearch</span></li></ol><div><br></div><div>Our terms are:</div><ul><li>4-5 days per week; 2 months project with opportunity to extend for more months depending on results</li><li>From £400 per day</li><li>Must be able to work from our London office once a week</li></ul>  </div>  <div>    <h4>Apply for this position</h4>    <p>Please send your resume and a brief paragraph each on why you want to work with Streamhub, and what you would bring to the table. Email: <a href=&quot;mailto:%68%69%72%69%6e%67@%73%74%72%65%61%6d%68%75%62.%63%6f.%75%6b&quot; rel=&quot;nofollow&quot;>hiring@streamhub.co.uk</a></p>  </div>",
    "urlFetchPending": false,
    "published": "Apr 26, 2017 4:39:32 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "2d9BkTq7RYSp90FtbGDWzA",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/131734/data-science-bootcamp-mentor-thinkful-inc?a=Ib7ZnszmKRO",
    "title": "Data Science Bootcamp Mentor at Thinkful Inc. () (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Data Science Bootcamp Mentor</a>                            </h2>                        </div><a rel=&quot;nofollow&quot;>Thinkful Inc.</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>Mentor aspiring Data Scientists enrolled in&nbsp;Thinkful's Data Science Flexible Bootcamp!</p><p>The intensive, 4-6 month course helps students become self-sufficient data scientists capable of advancing their skills as they grow in their careers for years to come. As a Mentor, you will meet work one-on-one with your&nbsp;student(s), meeting 3x a week as they build projects and master data science fundamentals.</p><p>Thinkful Mentors work remotely and set their own hours. The role is a great fit for experts who genuinely love their work and want to share their expertise, experience, and themselves with awesome students and fellow mentors from around the globe!</p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><ul><li><strong>Proficiency in Python is Required</strong></li><li><strong>Professional Experience as a Data Scientist or in Similar&nbsp;Role</strong></li></ul><ul><li>Experienced with Python Data Science Toolkit (Panda,&nbsp;NumPy, Matplotlib, Scikit-Learn, Beautiful Soup)</li><li>Experience with machine learning, Hadoop, Hive, Pig, Spark, MapReduce, distributed computing, and AWS</li><li>Professional experience in a field that utilizes modern day&nbsp;data science tools and methodologies</li><li>Able to explain&nbsp;mathematical concepts involved in statistics,&nbsp;probability and linear algebra</li><li>Experience with Git and GitHub</li><li>Involvement in local community (i.e. meet-ups) a plus</li></ul><ul><li>A desire to keep your skills sharp by teaching others</li><li>A passion for sharing best practices</li><li>Excellent communication skills</li><li>Infectious enthusiasm for development and sharing your craft with others</li><li>Previous corporate training or teaching experience a plus&nbsp;</li></ul>            </div>            <h3>                About Thinkful Inc.            </h3>            <div><p>At Thinkful relationships drive learning. We advance careers by pairing students with a&nbsp;1-on-1 mentors who pair program and teach best practices as students&nbsp;learn through a&nbsp;project-driven curriculum. Thinkful was recently ranked the best coding school&nbsp;by Course Report, the industry's leading publication. While we claimed the top spot in all five of their categories, what excites me most is that we have the #1 instructor team. A team comprised of 300 data scientists programmers and designers in over 35 countries.</p><p>Read what one of our awesome mentors says about mentoring with Thinkful, here:&nbsp;<a href=&quot;https://www.coursereport.com/schools/thinkful#/news/mentor-spotlight-jason-humphrey-of-thinkful&quot; rel=&quot;nofollow&quot;>https://www.coursereport.com/schools/thinkful#/news/mentor-spotlight-jason-humphrey-of-thinkful</a></p>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_python",
      "TZ_America",
      "TECH2_amazon-web-services",
      "TECH2_pandas",
      "TECH2_hadoop",
      "TECH2_machine-learning",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": [
      "amazon-web-services",
      "hadoop",
      "machine-learning",
      "pandas",
      "python"
    ]
  },
  {
    "id": "-zTvSD8gRWWUnITDOS4BsQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/136814/machine-learning-scientist-freelance-research-square?a=JSKUHv3RR7i",
    "title": "Machine Learning Scientist (Freelance) at Research Square (Durham, NC) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Machine Learning Scientist (Freelance)</a>                            </h2>                        </div><a rel=&quot;nofollow&quot;>Research Square</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p><a href=&quot;http://www.researchsquare.com/&quot; rel=&quot;nofollow&quot;>Research Square</a>&nbsp;is the leader in ethical author-oriented solutions in the world of academic publishing. &nbsp;We are home to the <a href=&quot;http://www.aje.com/&quot; rel=&quot;nofollow&quot;>American Journal Experts (AJE)</a>, which provides solutions that help researchers communicate their work so they can get back to making discoveries.</p><p>We currently are seeking a <em>freelance</em> <strong>Machine Learning Scientist</strong> to help us improve and maximize our use of TensorFlow for our language services. This will be a project-based contract with the goal of optimizing a sequence-to-sequence TensorFlow model for a natural language application.&nbsp;</p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><p><strong>Requirements for this role include:</strong></p><ul><li>Demonstrated experience with deep learning in the TensorFlow framework</li><li>Experience building TensorFlow apps that utilize multiple GPUs&nbsp;</li><li>Proficiency in working with large datasets</li><li>Experience coding in Python&nbsp;</li><li>Strong communication skills</li><li>Located in US, preferred</li></ul><p><strong>Our ideal candidate also has experience with the following:&nbsp;</strong></p><ul><li>Sequence-to-sequence models</li><li>Natural language processing</li><li>Amazon Web Services and/or Google Cloud</li><li>Familiarity with PHP, MySQL or other SQL databases, and R</li></ul>            </div>            <h3>                About Research Square            </h3>            <div><p><strong>Who we are</strong></p><p>We are a rapidly growing company based in Durham, NC, and our 200+ full-time employees are committed to making a difference in the world of research discovery, communication, and publication. We take our work to improve the lives of researchers very seriously, and we also believe in having fun doing so! &nbsp;Our work environment is casual and extremely flexible. We have a results-focused workplace; many enjoy coming in to our &quot;open office&quot; environment, and those who choose to work from home attend any required meetings via chat rooms and videoconferencing.&nbsp;</p><p><strong>Recent Company Awards</strong></p><ul><li>Inc. 5000 (2012, 2013, 2014, 2015)</li><li>Fast 50 (2013, 2014)</li><li>Inc. Hire Power (2013)</li><li>Build 100 (2013)</li><li>Sloan Award for Workplace Flexibility (2011, 2012)</li><li>When Work Works Award (2014, 2016)</li><li>NC Parenting Magazine’s Family Friendly 50 (2013, 2014)</li></ul>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_python",
      "TECH2_google-cloud-platform",
      "TZ_America",
      "TECH2_php",
      "TECH2_tensorflow",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data",
      "TECH2_mysql"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "google-cloud-platform",
      "mysql",
      "php",
      "python",
      "tensorflow"
    ]
  },
  {
    "id": "e2wEvBGvT1enERhE1zmVCw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/126469/big-data-engineer-100-remote-but-must-be-hotjar?a=GpEzpkuJ4Os",
    "title": "Big Data Engineer (100% Remote but must be between UTC-2 and UTC+7) at Hotjar () (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Big Data Engineer (100% Remote but must be between UTC-2 and UTC+7)</a>                            </h2>                                <span>                                            €60k - 80k                                                </span>                        </div><a rel=&quot;nofollow&quot;>Hotjar</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p><em><strong>Note:&nbsp;</strong>Although this is a remote position, we are currently&nbsp;only&nbsp;seeking candidates in </em><em>timezones b</em><em>etween UTC-2 and UTC+7.</em></p><p>Hotjar is looking for a driven and ambitious <strong>Big Data Engineer</strong> to support and expand our cloud-based infrastructure used by thousands of sites around the world. The Hotjar infrastructure currently processes more than <strong>10,0</strong><strong>00&nbsp;API requests per second</strong>, delivers over a billion pieces of static content every week and hosts databases well into <strong>terabyte-size</strong> ranges, making this an interesting and challenging opportunity. As Hotjar continues to grow rapidly,&nbsp;we are seeking an engineer who has experience dealing with high traffic cloud-based applications&nbsp;and can help Hotjar scale as our traffic multiplies.&nbsp;<br><br>This is an excellent career opportunity to join a fast growing remote startup in a key position.<br><br><strong>In this position, you will:</strong></p><ul><li>Be part of our DevOps&nbsp;team building and maintaining our web application and server environment.</li><li>Choose, deploy and manage tools and technologies to build and support a robust infrastructure.</li><li>Be responsible for identifying bottlenecks and improving performance of all our systems.</li><li>Ensure all necessary monitoring, alerting and backup solutions are in place.</li><li>Do research and keep up to date on trends in big data processing and large scale analytics.</li><li>Implement proof of concept solutions in the form of prototype applications.</li></ul>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><ul><li>4 or more years of experience working with Big Data, system administration and DevOps.</li><li>Strong working knowledge of Amazon AWS.</li><li>Experience with Python, Nginx, PostgreSQL, Hadoop and Elasticsearch.</li><li>Experience in any of the following is considered to be an asset: Redis, Memcached, Continuous integration and deployment, Mercurial, Vagrant.</li></ul>            </div>            <h3>                About Hotjar            </h3>            <div><p>Hotjar is a rapidly growing startup that is giving thousands of website owners the tools needed to discover how their visitors are really using their website. We are looking for passionate and ambitious developers who can help us shape the product and company while growing with us.</p><p><strong>Culture at Hotjar:</strong></p><p>Headquartered on the beautiful island of Malta, in the “heart” of the Mediterranean, Hotjar is a young startup that embraces remote working and personal development.</p><p>Hotjar’s culture is driven by transparency, respect, open discussion, collaboration and blunt and direct feedback. In fact, we’re obsessed with communicating with our users as well as within the team. We hate bureaucracy and slow moving organizations –&nbsp;but we’re suckers for well-defined processes. We love lean, iterative improvements and success is measured by the value we create for our users.</p><p><strong>The Perks:</strong></p><ul><li><strong>Remote &amp; Flexible.</strong> Work from anywhere within a European timezone you choose.&nbsp;</li><li><strong>Ample Time Off.&nbsp;</strong>All team members get 40 days of paid planned leave/year...plus 10 sick days/year.</li><li><strong>Collaborate with prestigious organizations.</strong> Imagine what it will feels like to be part of a product that is used by companies like Time Inc, Lloyds Bank, Pingdom, Booking.com, Intuit and the Red Cross.</li><li><strong>Only the best hardware and software</strong>. Mac, PC or Linux –&nbsp;we will get you equipped with the best hardware and software available.</li><li><strong>Personal home office budget.</strong> Every Hotjar team member receives a €3000 home office setup budget. Upgrade your desk, chair or buy any peripherals you might need.</li><li><strong>Kindle + Personal development budget.</strong> Everyone receives a free kindle as well as direct management of their own personal development yearly budget of €500. Buy books, short courses or magazine subscriptions.</li><li><strong>Work with a very talented team.</strong> Our team has an impressive background building and optimizing products and businesses around the globe.</li><li><strong>Make a difference.</strong> Hotjar is ‘democratizing’ site analytics and feedback by making them affordable and easy to use for everyone around the world. We call it the ‘Hotjar revolution’.</li></ul>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_postgresql",
      "TECH2_python",
      "TECH2_nginx",
      "TZ_America",
      "TECH2_amazon-web-services",
      "TECH2_hadoop",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "amazon-web-services",
      "hadoop",
      "nginx",
      "postgresql",
      "python"
    ]
  },
  {
    "id": "r0Kg7ltYR4y31oXeHJewQA",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/135471/enterprise-data-architect-engineer-critical-mix?a=JqPzDG8XCw0",
    "title": "Enterprise Data Architect / Engineer at Critical Mix (Perrysburg, OH) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Enterprise Data Architect / Engineer</a>                            </h2>                        </div><a rel=&quot;nofollow&quot;>Critical Mix</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>Provides expertise in the definition, adoption and adherence to enterprise business intelligence and information architecture strategies, processes and standards. Responsible for delivering enterprise information architecture and technical / infrastructure architecture solutions for the enterprise data warehouse and end user reporting tools. This position provides technology direction, assistance and training to teams performing BI-related activities, both inside and outside IT Services. Will Support the development of the global BI strategic road map by conducting market analysis and technology reviews, and establishing strong partnerships with key vendors to enable road map implementation.</p><p><em><strong>If you feel that AWS S3 is a great solution as a datalake and leveraging EMR/Spark for data transformations into a Redshift data warehouse.&nbsp; Then Critical Mix is a great fit.</strong></em></p><p>Office Locations</p><p>&nbsp;Perrysburg, Oh -&nbsp; Dallas, Tx - Warwick, RI - San Francisco, CA</p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><p>Responsible for designing and maintaining complete data systems architecture for the Critical Mix organization on enterprise levels</p><ul><li>Develop and Implement road maps and phased plans that gradually increases the enterprise data architecture maturity level of Critical Mix over a multi-year period and support both current &amp; future state business processes</li><li>Sets the overall direction, policies and guidelines for the Enterprise Data Architecture function</li><li>Maintains an ongoing partnership with the business to apply in-depth knowledge of the business operations, strategies, priorities and information requirements to establish the technical direction at an enterprise view</li><li>Promotion and delivery of shared infrastructure and applications to reduce costs and improve information flows</li><li>Defines the system, technical, and application architectures, and in some instances the business systems/process architecture for major areas of development</li><li>Creates processes and standards to develop, maintain and integrate enterprise architectures within the process of strategy development and technology planning</li><li>Ensures appropriate technical standards, procedures, and governance are defined and followed</li><li>Ensures enterprise solutions are scalable and adoptable according to changing business needs</li><li>Maintains close working relationships with executive and business management to understand the Critical Mix strategy and requirements</li><li>Keep up to date with new technologies and actively implement innovative solutions</li></ul><p><strong>Requirements</strong></p><ul><li>Bachelor's degree required; preferably in MIS</li><li>8+ years experience in IT including experience with systems architecture methodologies</li><li>3+ years of hands-on development experience in more than one of these technologies: Oracle, SQL Server, NoSQL, Big Data Technologies like Hadoop, Redshift, Snowflake or other DW storage architecture, Chartio or similar, Informatica or similar.</li><li>Dimensional data modeling principles (star and snowflake schemas, denormalized data structures, slowly changing dimensions, etc.)</li><li>Physical data architectures: data warehouses, independent data marts</li><li>Data integration tools: ETL (extract, transform, load), CDC (change data capture)</li><li>Data quality, master data management, metadata management, collaboration and business process management Bachelors or above degree in Computer Science, or related discipline, or equivalent experience.</li></ul>            </div>            <h3>                About Critical Mix            </h3>            <div><p>Critical Mix provides easy access to highly-targeted global survey respondents, survey programming and data visualization services for market research and consulting firms. Driven by a passion for simplifying data collection, the team at Critical Mix is personally invested in giving clients the ultimate customer service experience.</p>            </div>                        <p><em>                    The <a href=&quot;http://www.joelonsoftware.com/articles/fog0000000043.html&quot; rel=&quot;nofollow&quot;>Joel Test</a> is a twelve-question measure of the quality of a software team.                </em>            </p>                                    </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_business-intelligence",
      "TZ_America",
      "TECH2_artificial-intelligence",
      "TECH2_amazon-web-services",
      "TECH2_analytics",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TECH2_bigdata",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "amazon-web-services",
      "analytics",
      "artificial-intelligence",
      "bigdata",
      "business-intelligence"
    ]
  },
  {
    "id": "JOW2p2HXQLe_KkGnOLRBHg",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/130999/ruby-developer-uk-based-api-focus-learn-big-deepcrawl?a=HVQsqIVGiyY",
    "title": "Ruby Developer (UK based) - API focus, learn big data at DeepCrawl (Greater London, UK) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Ruby Developer (UK based) - API focus, learn big data</a>                            </h2>                                <span>                                            £40k - 60k                                                 | Equity                                                </span>                        </div><a rel=&quot;nofollow&quot;>DeepCrawl</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>At DeepCrawl, we pride ourselves in building our success on genuine, insightful relationships with our users. We originally built our product to help us solve our clients’ problems as consultants -- and that instinct for knowing our users, and yearning to help them, still powers our success as a software product.</p><p>Our sophisticated web crawler helps some of the world's top companies improve their web sites by giving them deep, comprehensive insights into their problems. Companies that make their business on the web depend on DeepCrawl to improve user experience, manage change, and improve indexability.&nbsp;In the end, it’s all about making the web a nicer place for users -- with fewer annoying gimmicks and more genuinely useful, well designed, easily found resources.</p><p>Our team is built foremost on constant learning, supporting each other, and loving working together. A great team environment supports us in building great technology. We’re always exploring new ideas and tech. Our stack now includes Ruby, PostgreSQL, MongoDB, a sophisticated API using Sinatra, and a front end in AngularJS.</p><p>Your role is to bring your passion for Ruby and other open-source technologies to improve our product, focusing on our API and broadening to other areas of the system. &nbsp;You'll have great opportunities to learn about large-scale crawling and data processing.</p><p>This position is available to remote candidates anywhere in the UK, and to London candidates who can work in our London office.</p><p>Responsibilities will include:</p><ul><li>Join a small but growing team of developers who love working together to improve design, usability, reliability, scalability, and clarity of code, all with the end goal of satisfying our many users and making the web a better place</li><li>Learn new technologies and practices, and provide support for other developers</li><li>Collaborate with product owners and other developers to write technical specifications for new features</li><li>Implement great new features that our users will love, using Ruby and other technologies</li><li>Design improvements to our API and implement new API features</li><li>Help improve test coverage and testability of code</li><li>Help improve internal tools for monitoring, testing, bug fixing, and administration</li></ul>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><p><strong>Essential:</strong></p><ul><li>A positive character and an ability to collaborate well with others</li><li>Solid professional development experience</li><li>A deep knowledge of Ruby</li><li>Experience building REST APIs</li><li>Solid experience with modern testing practice</li><li>Excellent spoken and written English</li></ul><p><br><strong>Desirable:</strong></p><ul><li>Experience with Sinatra</li><li>Knowledge of&nbsp;PostgreSQL</li><li>Experience with Sequel</li></ul>            </div>            <h3>                About DeepCrawl            </h3>            <div><p>DeepCrawl is a sophisticated crawling service, used by the world’s largest brands and SEO agencies to find and monitor many problems with their sites -- ultimately making them easier to find and use. &nbsp;We believe that by providing powerful tools to improve web sites, we make the web a better place for everyone. &nbsp;We’ve got big ambitions to continue our fast growth and expansion into new markets, and be the tool of choice for SEO, digital marketing and beyond.</p>            </div>                        <p><em>                    The <a href=&quot;http://www.joelonsoftware.com/articles/fog0000000043.html&quot; rel=&quot;nofollow&quot;>Joel Test</a> is a twelve-question measure of the quality of a software team.                </em>            </p>                                    </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_ruby",
      "TECH2_sinatra",
      "TZ_America",
      "REMOTE1_100",
      "TECH2_api-design",
      "TITLE1_data",
      "TECH2_mysql",
      "TECH2_refactoring",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": [
      "api-design",
      "mysql",
      "refactoring",
      "ruby",
      "sinatra"
    ]
  },
  {
    "id": "CuNECC0MTlK72N93NQA9Cw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/130823/big-data-course-authoring-opportunity-at-pluralsight?a=HSbz8mbUg0g",
    "title": "Big Data Course Authoring Opportunity at Pluralsight (FREELANCE) at Pluralsight () (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Big Data Course Authoring Opportunity at Pluralsight (FREELANCE)</a>                            </h2>                        </div><a rel=&quot;nofollow&quot;>Pluralsight</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>Are you a software developer with <strong>Big Data</strong>&nbsp;experience? Are you interested in sharing your knowledge with other developers, virtual mentoring, building your personal brand, and earning extra income in the process? Want to partner with one of the fastest growing education companies in the world?</p><p>Pluralsight, the leader in online technology training for software developers, is currently looking for individuals to develop and teach online technical courses to subscribers around the world.&nbsp; We are specifically looking for developers with relevant <strong>Hadoop</strong>&nbsp;knowledge, passion, and experience. &nbsp;</p><p>As a Pluralsight Author, you'll collaborate with our curriculum and editorial teams to select and develop your own technical course. We'll work together to ensure your course is presented and marketed in the best way possible. Our author positions are freelance/part-time, enabling a very flexible work environment.</p><p>Join&nbsp;a community of recognized expert instructors that inspire, teach, and transform lives everyday by helping learners acquire skills that provide career opportunities and advancement.</p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><ul><li>You'll need a deep knowledge of <strong>Hadoop</strong>, and/or its related frameworks and libraries. &nbsp;Specifically: <strong>Spark, Hive, Flume, Sqoop, Flink</strong></li><li>Prior teaching experience not required but preferred</li><li>Enthusiasm for <strong>Hadoop</strong>, its ecosystem, and helping others learn&nbsp;</li><li>Interest in eLearning (previous audio/video experience helpful but not required)</li><li>Ability to commit to producing&nbsp;a course over a negotiated, reasonable time period</li></ul>            </div>            <h3>                About Pluralsight            </h3>            <div><p>Founded in 2004, Pluralsight is the global leader in online learning for professional software developers, IT specialists and creative technologists. As the world’s largest curated professional development platform, the company offers instant access to more than 4,000 courses authored by top experts. With customers in more than 150 countries, Pluralsight serves as a career catalyst, delivering hands-on, practical training for the most in-demand and understaffed jobs of today.&nbsp;</p><p>Apply to become a Pluralsight author. Please include the following information:&nbsp;<br>First Name:<br>Last Name:<br>Email:<br>Who is your main audience?&nbsp;<br>What topics are you interested in creating courses for?<br>Any prior online (or offline) teaching experience?&nbsp;</p><p>Are you a Pluralsight subscriber? If not, we recommend signing up for our free 10 day trial to see what we’re all about.</p>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TZ_America",
      "TECH2_apache-spark",
      "TECH2_hadoop",
      "REMOTE1_100",
      "TECH2_hive",
      "TECH2_flume",
      "WORKAUTH_US",
      "TECH2_sqoop",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "apache-spark",
      "flume",
      "hadoop",
      "hive",
      "sqoop"
    ]
  },
  {
    "id": "DTzXVo5zT6uMZBla1-zk1A",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/130296/like-machine-learning-or-ai-help-us-expand-or-kcura?a=HHebaMvXiUg",
    "title": "Like machine learning or A.I.? Help us expand or product's analytical features!! at kCura (Reston, VA) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Like machine learning or A.I.? Help us expand or product's analytical features!!</a>                            </h2>                                <span>                                            $80k - 130k                                                </span>                        </div><a rel=&quot;nofollow&quot;>kCura</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>kCura is a global company with team members who are driven by our customers to build exceptional software for them to use every day. Our product, called Relativity, handles large volumes of data and helps corporations, law firms, and government agencies solve their own unique data problems.</p><p>• Work with your team and other stakeholders to define, design, implement, test, document, and deliver quality software products in a fast paced environment utilizing Java, Scala, and other technologies as needed.</p><p>• Utilize sound engineering practices to deliver functional, reliable, secure, tested and maintainable software that satisfies stakeholders requirements.</p><p>• Work with the team to improve the scalability and performance of existing products.</p><p>• Positively contribute to the culture, well-being, and growth of each team member and the company at large by being a helpful and considerate team member and by adhering to the company’s core values.</p><p>The way we work together is centered on our core values&nbsp;of collaborating, communicating, pushing to exceed expectations (even our own), being humble, and having fun while we do it. We enjoy ourselves, <a href=&quot;https://www.kcura.com/about-us/kcura-gives/&quot; rel=&quot;nofollow&quot;>give back</a>, and work (and play) hard together. If this sounds like the place for you, check out the details of this position below.</p><p>As a Advanced Software Engineer and member of the Conceptual Analytics team you will share responsibility for the design, development, automated tests and delivery of the company’s core analytics technologies.&nbsp; You must have a passion for solving problems while delivering reliable, highly scalable, highly performant systems that kCura’s customers use on a daily basis.</p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><ul><li>3+ years of experience creating backend / server / algorithmic software utilizing Java and other languages that run on the JVM, but not including J2EE / JEE applications.</li><li>Ability to decompose complex problems, troubleshoot issues and communicate solutions to the team and other stakeholders.</li><li>Demonstrated self-motivation to work independently as well as part of a high performing, diverse team</li><li>Excellent written and verbal communication skils</li><li>Experience developing and maintaining distributed, elastic micro-service applications that self-heal in the event of failures.</li><li>Experience creating large-scale services and applications in distributed environments such as Azure and Amazon AWS</li><li>Experience in utilizing Docker to simplify deployment and development</li><li>Skilled in developing software for the JVM using Java, Scala and other JVM based languages</li><li>Proficiency on Linux, Git, REST, Akka, and CI tools (Jenkins, Bamboo)</li></ul>            </div>            <h3>                About kCura            </h3>            <div><p>Founded in 2001, <a href=&quot;https://kcura.com/corporate&quot; rel=&quot;nofollow&quot;>kCura</a> are the developers of Relativity, software for managing and analyzing electronic data during litigation and investigations. Headquartered in downtown Chicago with additional global locations, we focus on providing the best software we can, striving to always improve our products and the experience of our customer base, which includes the U.S. Department of Justice and more than 190 of the top 200 law firms in the United States. kCura has been ranked the 175th fastest-growing technology company in North America by Deloitte's Technology Fast 500 <a href=&quot;https://www2.deloitte.com/content/dam/Deloitte/us/Documents/technology-media-telecommunications/us-tmt-fast500-2014-ranking-list.pdf&quot; rel=&quot;nofollow&quot;>Deloitte/us/Documents</a>, as well as one of Chicago Tribune's Top Workplaces <a href=&quot;http://www.topworkplaces.com/frontend.php/regional-list/company/chicagotribune/kcura-corporation&quot; rel=&quot;nofollow&quot;>Topworkplaces</a>. Our team of driven, passionate, and talented individuals works collaboratively to provide a positive client experience and build a reputable name in a booming industry. We commit to hiring people who value collaboration, communication, and accountability as much as we do. To learn more about kCura, check out our video <a href=&quot;https://kcura.com/corporate/careers/kcura-culture&quot; rel=&quot;nofollow&quot;>corporate/careers</a> on kCura's unique culture.</p>            </div>                        <p><em>                    The <a href=&quot;http://www.joelonsoftware.com/articles/fog0000000043.html&quot; rel=&quot;nofollow&quot;>Joel Test</a> is a twelve-question measure of the quality of a software team.                </em>            </p>                                    </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TZ_America",
      "TECH2_scala",
      "REMOTE1_100",
      "TECH2_azure",
      "TECH2_rest",
      "TECH2_java",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "azure",
      "java",
      "rest",
      "scala"
    ]
  },
  {
    "id": "MUucnKRAQOyF4C_FCRzLdA",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/128568/senior-data-engineer-restless-bandit?a=H7ivTUmalXi",
    "title": "Senior Data Engineer at Restless Bandit (San Francisco, CA) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Senior Data Engineer</a>                            </h2>                        </div><a rel=&quot;nofollow&quot;>Restless Bandit</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>As a Senior Data Engineer you will collaborate with other engineers and data scientists to develop and maintain the machine learning (ML) data pipeline powering Restless Bandit's talent-rediscovery products. Your days will involve architecting, improving, and customizing Restless Bandit's ML infrastructure based on well-established data-processing systems like Hadoop or &nbsp;while also staying on top of the latest developments in big-data computation platforms.</p><p>Culture:</p><p>We work hard, but at the same time we have a laid-back, fun work environment. Some of the values that are important to us are:</p><p>&nbsp;- Helpfulness: we appreciate receiving help as well as giving it, and learning from each other as well as sharing knowledge and opinions.</p><p>- Professionalism: we trust each other and assume independence and autonomy along with responsibility.</p><p>- Curiosity: we’re interested in new ideas and alternative viewpoints; we prefer having our minds’ changed by a discussion to ‘winning’ a debate.</p><p>- Relaxedness: we believe that consistent unhurried problem-solving produces better solutions than frantic ‘fire drills.’</p><p>- Fun: we're thick as thieves. We have lunch together every day, host regular (and spontaneous) &nbsp;happy hours, and participate in professional meet-ups.</p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><p>An ideal candidate has:</p><p>- 3+ years' experience building high-performance, large-scale, distributed server applications and reliable software.</p><p>- 2+ years' experience with distributed computing frameworks (Spark, Hadoop, Storm)</p><p>- 2+ years' experience with a compiled language (Go, C, C++, Java)</p><p>- 1+ years' experience writing production Python code</p><p>- 2+ years' experience writing effective unit tests against production code</p><p>- strong opinions about programming best practices coupled with flexibility in the face of cogent opposing opinions</p><p>- appreciation for data munging and the dirty aspects of the world of data processing.</p>            </div>            <h3>                About Restless Bandit            </h3>            <div><p>Restless Bandit is a well-funded San Francisco startup focused on helping companies identify and retain the best people in the exploding &quot;war-for-talent&quot; space. This is an exciting, near ground floor opportunity with a group of people with extensive experience in HR tech. Our founders were executives at Bright, an HR tech company acquired last year by LinkedIn for $130M and our data scientists and engineers come from Bright, LinkedIn, and Google, and have extensive experience applying big-data methods to the hiring domain.</p>            </div>                        <p><em>                    The <a href=&quot;http://www.joelonsoftware.com/articles/fog0000000043.html&quot; rel=&quot;nofollow&quot;>Joel Test</a> is a twelve-question measure of the quality of a software team.                </em>            </p>                                    </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_python",
      "TZ_America",
      "TECH2_amazon-web-services",
      "TECH2_hadoop",
      "TECH2_spark",
      "REMOTE1_100",
      "TECH2_elasticsearch",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "amazon-web-services",
      "elasticsearch",
      "hadoop",
      "python",
      "spark"
    ]
  },
  {
    "id": "IJsXxgR1RNOcjEZosOf5bw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "jobmote",
    "url": "https://jobmote.com/jobs/5012-big-data-engineer-100-remote-but-must-be-between-utc-2-and-utc-7",
    "title": "Big Data Engineer (100% Remote but must be between UTC-2 and UTC+7)",
    "content": "Big Data Engineer (100% Remote but must be between UTC-2 and UTC+7) - Jobmote<div><div><h2>Big Data Engineer (100% Remote but must be between UTC-2 and UTC+7)</h2><h5>Hotjar</h5></div></div><div><div><div><div><div><ul><li>4 or more years of experience working with Big Data, system administration and DevOps.</li><li>Strong working knowledge of Amazon AWS.</li><li>Experience with Python, Nginx, PostgreSQL, Hadoop and Elasticsearch.</li><li>Experience in any of the following is considered to be an asset: Redis, Memcached, Continuous integration and deployment, Mercurial, Vagrant.</li></ul><p>Hotjar is a rapidly growing startup that is giving thousands of website owners the tools needed to discover how their visitors are really using their website. We are looking for passionate and ambitious developers who can help us shape the product and company while growing with us.</p><p><strong>Culture at Hotjar:</strong></p><p>Headquartered on the beautiful island of Malta, in the “heart” of the Mediterranean, Hotjar is a young startup that embraces remote working and personal development.</p><p>Hotjar’s culture is driven by transparency, respect, open discussion, collaboration and blunt and direct feedback. In fact, we’re obsessed with communicating with our users as well as within the team. We hate bureaucracy and slow moving organizations –&nbsp;but we’re suckers for well-defined processes. We love lean, iterative improvements and success is measured by the value we create for our users.</p><p><strong>The Perks:</strong></p><ul><li><strong>Remote &amp; Flexible.</strong> Work from anywhere within a European timezone you choose.&nbsp;</li><li><strong>Ample Time Off.&nbsp;</strong> All team members get 40 days of paid planned leave/year...plus 10 sick days/year.</li><li><strong>Collaborate with prestigious organizations.</strong> Imagine what it will feels like to be part of a product that is used by companies like Time Inc, Lloyds Bank, Pingdom, Booking.com, Intuit and the Red Cross.</li><li><strong>Only the best hardware and software</strong>. Mac, PC or Linux –&nbsp;we will get you equipped with the best hardware and software available.</li><li><strong>Personal home office budget.</strong> Every Hotjar team member receives a €3000 home office setup budget. Upgrade your desk, chair or buy any peripherals you might need.</li><li><strong>Kindle + Personal development budget.</strong> Everyone receives a free kindle as well as direct management of their own personal development yearly budget of €500. Buy books, short courses or magazine subscriptions.</li><li><strong>Work with a very talented team.</strong> Our team has an impressive background building and optimizing products and businesses around the globe.</li><li><strong>Make a difference.</strong> Hotjar is ‘democratizing’ site analytics and feedback by making them affordable and easy to use for everyone around the world. We call it the ‘Hotjar revolution’.</li></ul></div></div><div></div></div></div></div>",
    "urlFetchPending": false,
    "published": "Apr 29, 2017 6:08:51 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "9UTI0wsARJKOF3o2IblyIw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "authenticjobs.com",
    "url": "https://authenticjobs.com/jobs/28979?r=rss2",
    "title": "Hotjar: Big Data Engineer (Europe)",
    "content": "<p>Full-time</p><p><strong>(Remote)</strong> </p><p><em><strong>Note: </strong>Although this is a remote position, we are currently only seeking candidates in </em><em>timezones</em><em> between UTC-2 and UTC+7.</em></p><p>Hotjar is looking for a driven and ambitious <strong>Big Data Engineer </strong>to support and expand our cloud-based infrastructure used by thousands of sites around the world. The Hotjar infrastructure currently processes more than 83<strong>00 API requests per second</strong>, delivers over a <strong>billion</strong> pieces of static content every week and hosts databases well into <strong>terabyte-size</strong> ranges, making this an interesting and challenging opportunity. As Hotjar continues to grow rapidly, we are seeking an engineer who has experience dealing with high traffic cloud based applications and can help Hotjar scale as our traffic multiplies. <br><br>This is an excellent career opportunity to join a fast growing remote startup in a key position.<br><br><strong>In this position, you will:</strong></p><ul><li>Be part of our DevOps team building and maintaining our web application and server environment.</li><li>Choose, deploy and manage tools and technologies to build and support a robust infrastructure.</li><li>Be responsible for identifying bottlenecks and improving performance of all our systems.</li><li>Ensure all necessary monitoring, alerting and backup solutions are in place.</li><li>Do research and keep up to date on trends in big data processing and large scale analytics.</li><li>Implement proof of concept solutions in the form of prototype applications.</li></ul><p><strong>Compensation Range</strong></p><p>The budgeted compensation range for this role is €60,000 to €80,000 annually. Ranges are based on market research and are equitable to other roles within Hotjar. The actual compensation offered to a successful candidate will be based on relative experience and skills. At this time we are only able to provide official employment status to those located in Malta. All other candidates will join our team as full-time consultants and will be responsible for paying any taxes or applicable fees where they reside. </p><h3>Requirements</h3><ul><li>4 or more years of experience working with Big Data, system administration and DevOps.</li><li>Strong working knowledge of Amazon AWS.</li><li>Experience with Python, Nginx, PostgreSQL, Hadoop and Elasticsearch.</li><li>Experience in any of the following is considered to be an asset: Redis, Memcached, Continuous integration and deployment, Mercurial, Vagrant.</li><li>Must submit to a background check confidentially processed by our third party</li></ul><p></p><p><strong>Apply:</strong> </p>",
    "urlFetchPending": false,
    "published": "Mar 13, 2017 11:57:20 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "uGj4q5UcRPGc9U17Ua7wRQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "wfh.io",
    "url": "https://www.wfh.io/jobs/3579-java-big-data-engineer-spinn3r-inc",
    "title": "Java “big data” Engineer @ Spinn3r Inc",
    "content": "<h2>              Java “big data” Engineer<small> @ Spinn3r Inc</small>          </h2><dl>          <dt></dt>          <dd><p></p></dd>          <dt></dt>          <dd><p>2017-03-22 20:15</p></dd>          <dt><span>Description</span></dt>          <dd><p><strong>Company</strong></p><p>Spinn3r is a social media and analytics company looking for a talented Java “big data” engineer. </p><p>As a mature, ten (10) year old company, Spinn3r provides high-quality news, blogs and social media data for analytics, search, and social media monitoring companies.   We’ve just recently completed a large business pivot, and we’re in the process of shipping new products so it's an exciting time to come on board!</p><p><strong>Ideal Candidate</strong></p><p>We're looking for someone with a passion for technology, big data, and the analysis of vast amounts of content; someone with experience aggregating and delivering data derived from web content, and someone comfortable with a generalist and devops role.  We require that you have a knowledge of standard system administration tasks, and have a firm understanding modern cluster architecture.  </p><p>We’re a San Francisco company, and ideally there should be least a 4 hour overlap with the Pacific Standard Time Zone (PST / UTC-8).  If you don't have a natural time overlap with UTC-8 you should be willing to work an alternative schedule to be able to communicate easily with the rest of the team.<br>Culturally, we operate as a “remote” company and require that you’re generally available for communication and are self-motivated and remain productive.</p><p>We are open to either a part-time or full-time independent contractor role.</p><p><strong>Responsibilities</strong></p><ul><li>Understanding our crawler infrastructure;</li><li>Ensuring top quality metadata for our customers. There's a significant batch job component to analyze the output to ensure top quality data;</li><li>Making sure our infrastructure is fast, reliable, fault tolerant, etc.  At times this may involve diving into the source of tools like ActiveMQ to understand how the internals work.  We contribute to Open Source development to give back to the community; and</li><li>Building out new products and technology that will directly interface with customers. This includes cool features like full text search, analytics, etc. It's extremely rewarding to build something from ground up and push it to customers directly. </li></ul><p><strong>Architecture</strong></p><p>Our infrastructure consists of Java on Linux (Debian/Ubuntu) with the stack running on ActiveMQ, Zookeeper, and Jetty.  We use Ansible to manage our boxes. We have a full-text search engine based on Elasticsearch which also backs our Firehose API.</p><p>Here's all the cool products that you get to work with:</p><ul><li>Large Linux / Ubuntu cluster running with the OS versioned using both Ansible and our own Debian packages for software distribution;</li><li>Large amounts of data indexed from the web and social media.  We index from 5-20TB of data per month and want to expand to 100TB of data per month; and </li><li>SOLR / Elasticsearch migration / install.  We’re experimenting with bringing this up now so it would be valuable to get your feedback.</li></ul><p><strong>Technical Skills</strong></p><p>We're looking for someone with a number of the following requirements:</p><ul><li>Experience in modern Java development and associated tools: Maven, IntelliJ IDEA, Guice (dependency injection);</li><li>A passion for testing, continuous integration, and continuous delivery;</li><li>ActiveMQ. Powers our queue server for scheduling crawl work;</li><li>A general understanding and passion for distributed systems;</li><li>Ansible or equivalent experience with configuration management; </li><li>Standard web API use and design. (HTTP, JSON, XML, HTML, etc.); and</li><li>Linux, Linux, Linux.  We like Linux!</li></ul><p><strong>Cultural Fit</strong></p><p>We’re a lean startup and very driven by our interaction with customers, as well as their happiness and satisfaction. Our philosophy is that you shouldn’t be afraid to throw away a week's worth of work if our customers aren’t interested in moving in that direction.</p><p>We hold the position that our customers are our responsibility and we try to listen to them intently and consistently:</p><ul><li>Proficiency in English is a requirement. Since you will have colleagues in various countries with various primary language skills we all need to use English as our common company language. You must also be able to work with email, draft proposals, etc. Internally we work as a large distributed Open Source project and use tools like email, slack, Google Hangouts, and Skype; </li><li>Familiarity working with a remote team and ability (and desire) to work for a virtual company. Should have a home workstation, and fast Internet access, etc.;</li><li>Must be able to manage your own time and your own projects.  Self-motivated employees will fit in well with the rest of the team; and</li><li>It goes without saying; but being friendly and a team player is very important.</li></ul><p><strong>Compensation</strong></p><ul><li>Salary based on experience;</li><li>We're a competitive, great company to work for; and</li><li>We offer the ability to work remotely, allowing for a balanced live-work situation.</li></ul></dd>          <dt></dt>            <dd>              <p>                              </p>            </dd>          <dt><span>Country</span></dt>          <dd><p>Anywhere</p></dd>        </dl>",
    "urlFetchPending": false,
    "published": "Mar 22, 2017 9:50:02 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "qRXrVBUqTEClJoQH0f3Uug",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "remoteok.io",
    "url": "https://remoteok.io/jobs/23270",
    "title": "Data Architect",
    "content": "<div><span>CafeMedia is looking for a seasoned and multi-talented dataarchitect to help us create our overarching enterprise data architecture, buildand manage the build-out of the underlying databases, implement tools and technologiesto serve business users, specify and build ETL processes, and implementgo-forward best practices for managing our database as we grow.</span><br></div><div></div><div></div><div>&nbsp;<br></div><div><span>CafeMedia is enormous and has enormous cool data sets thatYOU want to mess with!</span><br></div><div></div><div></div><div>&nbsp;<br></div><div><span>We have multiple large-scale and/or complex data sets thatlive independently that need to be holistically unified. The data sets include:</span><br></div><div></div><div></div><ul><li>Customer database<br></li><li>DFP<br></li><li>Ad Exchanges<br></li><li>Google Analytics<br></li><li>Custom Tracking Pixels<br></li><li>CMSs<br></li><li>Scraped data from a variety of third party platforms, APIs,scripts and data files<br></li></ul><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div><span><br></span></div><div><span><b>Job responsibilities:</b></span></div><div></div><div></div><ul><li>This is a mix of big-picture strategy (25%) and very Ã¢Â Â in theweedsÃ¢Â Â  execution work (75%) Ã¢Â Â  talking with the CEO and writing SQL<br></li><li>Full ownership of the process from end-to-end andsignificant autonomy to make the best decisions for the company<br></li><li>Interviewing business stakeholders across the company tounderstand all needs<br></li><li>Investigating all data sources to understand formats,structure, quality, etc<br></li><li>Proposing and selling-in the overarching data structure Ã¢Â Â and implementing that structure<br></li><li>Writing requirements and specifications around the ETL andaggregation processes Ã¢Â Â  and creating those processes<br></li><li>Setting best practices for the ingestion of new data andmanaging go-forward operations<br></li><li>Recommending and implementing user tools<br></li><li>Opportunity to choose platforms, tools and technologiesacross the entire project Ã¢Â Â  but weÃ¢Â Â re heavily biased to open sourcetechnologies and the Amazon Cloud for the underlying systems<br></li></ul><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div>&nbsp;<br></div><div></div><div><span><b>Requirements:</b></span></div><div></div><ul><li>10+ years of experience in data architecture/modeling<br></li><li>Background in digital content/digital advertising asignificant benefit<br></li><li>Self-starter<br></li><li>Excellent communicator<br></li><li>Great dancer (just kidding)<br></li><li>Great karaoke performer (not kidding)<br></li></ul><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><div></div><p><strong>To apply:</strong> Interested candidates should follow the below link to apply:<a href=&quot;http://cafemedia.applytojob.com/apply/bByYZqt37O/Data-Architect?source=we+work+remotely&quot; rel=&quot;nofollow&quot;>http://cafemedia.applytojob.com/apply/bByYZqt37O/Data-Architect?source=we+work+remotely</a>We regret that we are unable to respond to each resume. Only those individuals selected for interviews will be contacted. CafeMedia is an equal opportunity employer.</p>",
    "urlFetchPending": false,
    "published": "Apr 27, 2017 5:49:53 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "Yy4_adKATaqP1TMW9PaQew",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "remoteok.io",
    "url": "https://remoteok.io/jobs/23248",
    "title": "Spark / ElasticSearch Data Engineer (Contractor, UK only)",
    "content": "<div>Streamhub is a video analytics startup providing Data-As-A-Service to premium media businesses around the world. In short, we are a Google Analytics for VOD. If you have a knack for distributed systems and discovering value out of the mountains of viewing data we get from our audiences, then this is for you.</div><div><br></div><div>We are looking for an experienced senior big data engineer (contractor) with expertise in Spark and ElasticSearch. The job will be mostly remote but we ask you to come into our London office at the start and from time to time during the 2-3 month project.&nbsp;</div><div><br></div><div>You will play a key role in our big data engineering team, which is a 100% hands-on.In line with StreamhubÃ¢Â Â s ambitions, we are challenging to derive real-world value and structure from the vast range of Ã¢Â Â video contentÃ¢Â Â  and Ã¢Â Â audienceÃ¢Â Â  data all around us.</div><div><br></div><div>You can expect hard but rewarding work that brings your skills into the world. We have a lean, pragmatic and open culture that values communication and a focus on iterative development.&nbsp;</div><div><br></div><div><span>The Challenges:</span><br></div><ul><li>Design, implement and support new features for Elasticsearch and Cassandra</li><li>Build and support features for Spark, Hadoop.</li><li>Optimise the integration of new data sources, addition of business logic that provide further layers and dimensions to the data analysis</li></ul><div><br></div><div><span>We would need you to:</span></div><div><br></div><ul><li>Be deeply profficient and have 4+ years in object oriented development, preferably Java or Scala.</li><li>Expert in Elasticsearch, including its operational aspects.</li><li>Have 2+ years of hands-on experience with Spark, Cassandra and the Hadoop ecosystem technologies</li><li>Experience with AWS big data stack</li><li>Must be a great team player with proven reading, writing and speaking of English</li><li>Experience with DSPs, SSPs, DMPs are a big plus</li><li>Machine learning and predictive algorithms a further plus</li></ul><div><br></div><div>Our technology stack that will keep you busy</div><ol><li><span>Scala/Java</span></li><li><span>Apache Spark or Hadoop stack</span></li><li><span>AWS big data stack</span></li><li><span>Cassandra</span></li><li>Apache Nifi</li><li><span>Elasticsearch</span></li></ol><div><br></div><div>Our terms are:</div><ul><li>4-5 days per week; 2 months project with opportunity to extend for more months depending on results</li><li>From Ã Â£400 per day</li><li>Must be able to work from our London office once a week</li></ul><p><strong>To apply:</strong> Please send your resume and a brief paragraph each on why you want to work with Streamhub, and what you would bring to the table. Email: <a href=&quot;mailto:%68%69%72%69%6e%67@%73%74%72%65%61%6d%68%75%62.%63%6f.%75%6b&quot; rel=&quot;nofollow&quot;>hiring@streamhub.co.uk</a></p>",
    "urlFetchPending": false,
    "published": "Apr 26, 2017 4:39:32 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "qL7hDNVASc-4hB9-uqRNzA",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "remoteok.io",
    "url": "https://remoteok.io/jobs/23037",
    "title": "Adobe Captivate / E-Learning Adult Learning Content Developer",
    "content": "<div>This Job is a Remote, freelance role. Ã Â&nbsp;We seek candidates who's first language is English, however you can be located anywhere in the world.</div><div><br></div><div><b>Adobe Captivate / E-Learning Adult Learning Content Developer</b></div><div><b><br></b></div><div>Title says it all. Ã Â&nbsp;We have content in the form of PowerPoint slides that we need to convert to engaging, effective e-learning modules using Adobe Captivate.</div><div><br></div><div>Ideal candidate will have proven knowledge, training, and experience developing effective, highly interactive e-Learning modules for adult learners.</div><div><br></div><div>Content of our E-Learning modules covers modern digital product development, organizational culture, management, and leadership.</div><div><br></div><div>Ideal candidates will also have proven graphic design skills using tools like PhotoShop, In-design, etc.</div><div><br></div><div>Lastly, preference will be given to freelancers comfortable establishing a standard rate per 1 hour of delivered E-Learning content.</div><div><br></div><div><b>To be considered, <i>candidates MUST have an online portfolio of E-Learning content</i></b><i> </i>they've previously developed that we can review to evaluate your skill and experience.</div><div><br></div><div><b>Ã Â&nbsp;</b></div><p><strong>To apply:</strong> <a href=&quot;mailto:%62%72%61%64.%6d%75%72%70%68%79@%67%65%61%72%73%74%72%65%61%6d.%63%6f%6d&quot; rel=&quot;nofollow&quot;>brad.murphy@gearstream.com</a></p>",
    "urlFetchPending": false,
    "published": "Apr 13, 2017 9:22:47 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "Uxib7GVLSCiPO7QEM9fBgw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "remoteok.io",
    "url": "https://remoteok.io/jobs/23002",
    "title": "Data Engineer",
    "content": "<p>At Toptal, we measure everything and always rely on data to guide all of our initiatives, including both our long-term strategy and our day-to-day operations.</p><br><p>As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts, and support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have significant impact on our velocity. We, in turn, will give you autonomy and freedom to turn your ideas into reality.</p><br><p>This is a remote position that can be done from anywhere. However, we do things like <a href=&quot;http://www.toptal.com/remote/the-ultimate-remote-culture&quot; rel=&quot;nofollow&quot;>rent out hotels in Africa or mansions in Thailand</a>, and you will certainly be invited to come work with us.</p>",
    "urlFetchPending": false,
    "published": "Apr 12, 2017 6:02:51 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "ZROxcQcOREGPWGX-FAvEKQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "remoteok.io",
    "url": "https://remoteok.io/jobs/22857",
    "title": "Data Engineer",
    "content": "<div>Our company runson data. We have a&nbsp;<strong><em>lot</em></strong>&nbsp;of data, and a lot of peoplelooking at that data.</div><div><br></div><div>For most areas ofour business, there are systems, processes, and tools in place to extract,transform, load, read, visualize, and query data from many different sources.</div><div>Recently, a newbusiness unit joined our family. This business is thriving and its customersare very happy. There is a&nbsp;<strong><em>lot</em></strong>&nbsp;of data here, too, butthere are few systems, processes, or tools in place to handle this data.</div><div><br></div><div>This is where youcome in.</div><div><br></div><div>We are kicking offthe process of building new reporting systems for this business, and you willbe the first data engineer working on this project.</div><div><br></div><div>This is a uniqueopportunity to have a lot of control over a greenfield project, yet at the sametime join a well established team with many other experienced engineers whohave solved similar problems.</div><div><br></div><div>This is a verylong term project, very well funded, and we are committed to making the besttechnical choices while solving problems that will increase business value andmake many people very happy.</div><div><br></div><div><strong>Specifically, theperson hired for this role will:</strong></div><ul><li>Work closely withbusiness and engineering stakeholders to understand reporting needs</li><li>Work closely witha Data Architect to design a complicated reporting system</li><li>Work with otherengineers to choose a platform for implementing that reporting system (fullstack)</li><li>Create a suite ofprocesses for ingesting data from a dozen different internal and external datasources</li><li>Choose or buildquerying and visualization tools</li><li>Rinse, repeat, andhave lots of fun with data!</li></ul><div><strong><br></strong></div><div><strong>Qualifications:</strong></div><ul><li>Strong computerscience fundamentals</li><li>Softwareengineering experience</li><li>Experienceproducing and consuming event driven data</li><li>Experiencebuilding scalable and reliable data pipelines</li><li>Understanding ofstructured and unstructured data design and modeling</li><li>Passion for dataengineering automation and efficiency</li><li>Experience withtechnologies like PostgreSQL, MySQL, Spark, Hadoop</li><li>Familiar with AWSservices and data process frameworks</li><li>MS in ComputerScience or equivalent work experience</li><li>Knowledge of adserving platforms and online advertising systems is a huge plus.</li></ul><div><br></div><div>CafeMedia is an equal opportunity employer.</div><div>&nbsp;</div><div><br></div><p><strong>To apply:</strong> Please click the link below:<a href=&quot;http://cafemedia.applytojob.com/apply/xK34xw897b/Data-Engineer?source=we+work+remotely&quot; rel=&quot;nofollow&quot;>http://cafemedia.applytojob.com/apply/xK34xw897b/Data-Engineer?source=we+work+remotely</a></p>",
    "urlFetchPending": false,
    "published": "Apr 6, 2017 12:11:20 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "n6odGdYxSrCJDk-8HGVaYg",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/56760/spark-data-engineer-at-bitposter-automating-out-of-home/",
    "title": "Spark Data Engineer",
    "content": "*About the role*<br /><br />Bitposter is looking for a Data Engineer who is experienced in Spark who has worked in a product driven environment ideally.<br /><br />The Tech: <br />We have an existing framework for processing data on Apache Spark that feeds two further downstream systems: <br /><br />    - Druid database (http://druid.io)<br />    - Python/Django web application for serving RESTful APIs from a Postgresql database.<br /><br />We load csv data into HDFS for computationally intensive processing that currently takes many hours. Jobs are written in Python, though we are open to using Scala or possibly Java.<br /><br />*Experience*<br /><br />You will have a central voice in determining our data processing and analytics strategy including choice of technologies.<br /><br /><br />Required skills:<br /><br />* General knowledge of Hadoop and MapReduce technologies<br /><br />* Specific, substantive experience with Apache Spark: you must have good knowledge of Apache spark, preferably with very good Python or Scala experience. <br />   <br />    - Create and manage jobs<br />    - AWS EMR is our Spark implementation<br /><br />* Postgresql: this is our principle transactional data processing storage technology. <br /><br />* General knowledge of document-oriented databases. <br /><br />* Good data analytics experience. <br /><br /><br />*Additional Experience*<br /><br />Knowledge of Druid and MongoDB would be very helpful.<br /><br />*Salary*<br />£50,000 - £70,000 depending on experience<br /><br />NOTE: Working from home is an option for this role with occasional office visits (London based) and national applications are considered.<br /><br /><br />*A bit about us*<br /><br />Founded in 2013, Bitposter is the leading automated platform for trading out of home (OOH) media.<br /><br />Built to provide efficiencies and transparency in how OOH media is bought and sold today, Bitposter offers an easy to use cloud based environment where media owners can list both classic print and digital (DOOH) inventory, and buyers can leverage first and third party data to plan, negotiate, option and book activity in real-time.<br /><br />95% of OOH media owners, including: JCDecaux, Exterion, and Primesight, sell their inventory via the Bitposter platform; providing buyers with live availability across more than 400,000 classic and digital screens, across the UK. <br /><br />",
    "urlFetchPending": false,
    "published": "Apr 25, 2017 4:14:36 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "lINe6O42RrClOQlaWKi8rw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/56183/digital-data-analyst-fm-at-delivery-hero/",
    "title": "Digital Data Analyst (f/m)",
    "content": "Delivery Hero is building the next generation global online food-ordering platform. Our awesome international team already operates in over 40 countries worldwide to ensure hungry customers get the fastest way to their favorite takeaway food. The company has grown from its inception in 2011 to become the World's largest food-ordering network. This is an exciting time for Delivery Hero with rapid growth in markets and opportunities. Our ideal candidate will be enthusiastic, innovative and good at &quot;getting things done&quot;. You will play an active role in our future - an exciting job and a workplace in the heart of Berlin is waiting for you! <br /><br />The Delivery Hero Digital Analytics team is small, fast and dynamic. We are passionate about measuring, processing and analyzing reliable data for providing valuable insights to all brands of the Delivery Hero family for improving the user experience of our customer on all platforms, solve business questions and finding room for improving!<br />We are looking for a teammate to help turn user behavior into the right measures and ‘talking data’ for supporting the achieving their business goals. <br /><br />We value insights from data to drive for excellence in all business units and happily collaborate with all layers of our organisation to get the best approaches.<br /><br />You also must have the confidence and experience to explain and persuade others about web analytics. If this excites you, we can offer you the chance to grow and leave your mark on a company with explosive growth.<br /><br /><br />Your Mission:<br /><br />Boost the company to the next level of analytics by being a member a team that is making web analytics a strong part of a centralized data strategy<br />Establish and optimize the holistic web analytics approach for our brands and platforms all over the company <br />Leverage insights from web analytics data by providing analyses, reports and dashboards for marketing, product management and other departments<br />Collaborate with other team members, marketeers, product managers and additional stakeholders to align on elaborated web analytics specifications for their requirements that fit<br />Communicate ideas and knowledge within the the team and to stakeholders to foster a data driven mindset<br />Developing strong hypotheses, independently solve problems, and share game-changing insights to drive growth<br />Your Heroic Skills:<br /><br />1 - 2 years’ experience in a data analytics role.<br />Previous experiences with data processing tools like Google BigQuery is a big plus<br />You have good understanding of web technologies like Javascript, HTML & CSS.<br />Previous experience with tracking technologies (Google Tag Manager, Google Analytics, Adobe Analytics, Webtrekk, etc) is mandatory<br />You are structured, analytical, detail oriented and precise. You have strong critical thinking and an affinity for numbers.<br />You work independently, as well as part of a team. You are able to prioritize, handle multiple projects at the same time and work towards tight deadlines.<br />We offer you:<br /><br />A dynamic, fast paced environment.<br />A team that want to excel and make customers happy.<br />Collaborative environment with experienced colleagues.<br />Dynamic international office environment with English as a working language.<br />Flexible and agile company with flat hierarchy.<br /><br /><br />Have we caught your attention? Then please send us your application including cover letter, CV, salary expectations and earliest starting date. We’re looking forward to your application!",
    "urlFetchPending": false,
    "published": "Apr 7, 2017 10:58:25 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "YR3qJcuBRxeavBhojtOXbw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55802/machine-learning-engineer-at-ravelin/",
    "title": "Machine Learning Engineer",
    "content": "_Ravelin prevents fraud and protects margins for online businesses. Companies all over the world are accepting more transactions with fewer chargebacks thanks to our machine learning-based approach to fraud prevention._ <br /><br />Ravelin is looking for a mid/senior machine learning engineer to work on our fraud detection platform. You will joining the 6-person detection team to help develop our ML technology that blocks fraudsters in real-time. This role reports to the CIO. We're situated in beautiful Clerkenwell, close to Farringdon station, 3 food markets and too many coffee shops to count.<br /><br />You will be involved in research, analysis, modelling, infrastructure and helping to define the direction of the company. The projects you could be working on include: active learning, transfer learning, deep learning, anomaly detection, behavioural modelling, featuring engineering, graph analysis and device fingerprinting.<br /><br />What we’re doing is pretty complex. Fraudsters move quickly, changing tactics all of the time. We have to build robust models that are capable of updating their beliefs in real time when they encounter new methods of fraud: our clients expect us to be one step ahead of fraud, not behind. You’ll be given the time, space and equipment you need to research and develop groundbreaking new methods for detecting fraudsters on the web using our rich data.<br /><br />REQUIREMENTS <br />* You have trained, evaluated and deployed models to production. <br />* Experience of writing and testing robust production code. <br />* You have dealt with large volumes of semi-structured data. <br />* Pragmatic, autonomous and creative.<br /><br />Experience in any of the following is a bonus but it’s not required: python, scikit-learn and the python data stack, SQL, go, google cloud, big query, agile methodologies and linux.<br /><br />APPLY <br />No recruiters please. You will be asked to do a short take-home test and then subsequently invited to meet the team for a face-to-face interview.<br /><br />We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",
    "urlFetchPending": false,
    "published": "Mar 28, 2017 12:10:43 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "r31F1E5hQx2pNRvfAclnDw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55789/data-risk-engineer-at-iwoca-ltd/",
    "title": "Data & Risk Engineer",
    "content": "As a Data & Risk Engineer at iwoca you will be a crucial member of a team responsible for developing a bespoke lending platform that has recently been accredited as being far more advanced than any other product on the market. We have ambitious plans to transform the face of SME lending and we have already made waves in the industry.<br /><br />Our Data & Risk Engineer develop cutting edge algorithms using statistical modelling and machine learning techniques. Using Python (Pandas, NumPy and SciPy) they work in a fast paced and dynamic environment to produce data driven solutions. Our Tech & Analytics team works in a relaxed, collaborative and flexible environment, where everyone has the opportunity to make a significant contribution to the business. As an iwocan, you will be expected to derive meaningful and actionable insights from our data as well as develop the innovative technologies behind our platform.<br /><br />We are looking for Engineer who love to code and get their hands dirty with raw data. It doesn't really matter whether people call you an Engineer, Physicist or Mathematician, what matters to us is that you are a curious and continuous learner with a hunger to achieve the very best in what you do.<br /><br />About us:<br />* We are a team of 7 with a real love for mathematical and statistical modelling<br />* We all play a significant role in specifying the problem and thinking through all possible solutions<br />* Our development cycle is tight with Agile-ish processes that means features or projects go live in days or weeks rather than months or years<br />* We are an eclectic bunch of Musicians, Rowers, Climbers, Skiers, Farmers and Cyclists<br /><br />We are looking for people with:<br />* Strong Numerate background, e.g Degree in Engineering, Mathematics, Physics or equivalent<br />* A background in Probability & Statistics<br />* Programming experience: Python, Java, C/C++ or similar<br />* Self-starter with ability to work autonomously in an unstructured environment<br />* Demonstrable experience using machine learning techniques<br />* Strong basis in computer-science fundamentals<br />* Experience with Pandas, NumPy, SciPy, R, Matlab, and/or SQL.<br />* Understanding of web technologies<br />* Experience in scorecard building and credit / fraud models<br /><br />*Benefits* <br />You will always have a say in the business and have the opportunity to really make a difference. We help all our employees become top performers and reward everyone in numerous different ways. Everyone at iwoca is smart, humble and motivated to do a good job so you are bound to love working here. We also offer the following:<br />* Pool and Ping Pong tables<br />* Great development programmes (paid conferences around Europe)<br />* Plenty of space to chill out<br />* Plenty of drinks and snack (healthy and unhealthy)<br />* Subsidised gym membership and restaurant discounts in Soho<br /> * Cycle2Work scheme for great value bikes<br />* Various sports teams including football and climbing<br />* A super smart team of really nice people<br />* Company retreats to great destinations<br />* Ability to work from home occasionally",
    "urlFetchPending": false,
    "published": "Mar 28, 2017 8:51:11 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "EFgrFvD2S9ag5_lVTHWggg",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55743/ruby-developer-uk-based-api-focus-learn-big-data-at-deepcrawl/",
    "title": "Ruby Developer (UK based) - API focus, learn big data",
    "content": "At DeepCrawl, we pride ourselves in building our success on genuine, insightful relationships with our users. We originally built our product to help us solve our clients’ problems as consultants -- and that instinct for knowing our users, and yearning to help them, still powers our success as a software product.<br /><br />Our sophisticated web crawler helps some of the world's top companies improve their web sites by giving them deep, comprehensive insights into their problems. Companies that make their business on the web depend on DeepCrawl to improve user experience, manage change, and improve indexability. In the end, it’s all about making the web a nicer place for users -- with fewer annoying gimmicks and more genuinely useful, well designed, easily found resources.<br /><br />Our team is built foremost on constant learning, supporting each other, and loving working together. A great team environment supports us in building great technology. We’re always exploring new ideas and tech. Our stack now includes Ruby, PostgreSQL, MongoDB, a sophisticated API using Sinatra, and a front end in AngularJS.<br /><br />Your role is to bring your passion for Ruby and other open-source technologies to improve our product, focusing on our API and broadening to other areas of the system.  You'll have great opportunities to learn about large-scale crawling and data processing.<br /><br />This position is available to remote candidates anywhere in the UK, and to London candidates who can work in our London office.<br /><br />*Responsibilities will include:*<br /><br />* Join a small but growing team of developers who love working together to improve design, usability, reliability, scalability, and clarity of code, all with the end goal of satisfying our many users and making the web a better place<br />* Learn new technologies and practices, and provide support for other developers<br />* Collaborate with product owners and other developers to write technical specifications for new features<br />* Implement great new features that our users will love, using Ruby and other technologies<br />* Design improvements to our API and implement new API features<br />* Help improve test coverage and testability of code<br />* Help improve internal tools for monitoring, testing, bug fixing, and administration<br /><br /><br /><br />h2. Skills & Requirements<br /><br />*Essential:*<br /><br />* A positive character and an ability to collaborate well with others<br />* Solid professional development experience<br />* A deep knowledge of Ruby<br />* Experience building REST APIs<br />* Solid experience with modern testing practice<br />* Excellent spoken and written English<br /><br />*Desirable:*<br /><br />* Experience with Sinatra<br />* Knowledge of PostgreSQL<br />* Experience with Sequel<br /><br /><br /><br />h2. About DeepCrawl<br /><br />DeepCrawl is a sophisticated crawling service, used by the world’s largest brands and SEO agencies to find and monitor many problems with their sites -- ultimately making them easier to find and use.  We believe that by providing powerful tools to improve web sites, we make the web a better place for everyone.  We’ve got big ambitions to continue our fast growth and expansion into new markets, and be the tool of choice for SEO, digital marketing and beyond.",
    "urlFetchPending": false,
    "published": "Mar 27, 2017 11:32:45 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "Fg3cu2vhQ9WWjRdnN9acRw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55648/senior-data-scientist-london-sparkbeyond-at-sparkbeyond/",
    "title": "Senior Data Scientist @ London - SparkBeyond",
    "content": "Position Overview<br /><br />We seek an outstanding Data Scientist to join our team in London, taking our unique and disruptive technology to the market, while making significant impact on the product’s roadmap.<br /><br />If you qualify and would like to join one of the fastest growing technology companies and have a real impact in helping to tackle and solve some of the world’s largest problems, please reach out to us.<br /><br />SparkBeyond offers an AI-powered engine that automates the most creative part of the data-science lifecycle: feature engineering.<br />At SparkBeyond you’ll work hand-in-hand with Fortune 100 companies and non-for-profit organizations, helping them solve their toughest business problems using machine learning and data science.<br /><br />Our mission is to automate and scale the creative part of Data Science, empowering Data Scientists, Researchers and Business Analysts with a machine that constantly analyzes the data and surfaces discoveries, automatically.<br /><br />Responsibilities include:<br />Working directly with our customers’ Data Science teams to help them to be successful with the SparkBeyond Discovery Platform; our Data Scientists get to work on solving various problem types across various domains (Financial Risk, Telco Churn, Maintenance Failure, Marketing, Oil & Gas Exploration, Logistics Delays and more…)<br />Translating Data Science to business values<br />Consulting on our approach to Data Science using our automated AI-powered research engine.<br />Helping to shape new functionality for our unique offering<br />Working directly with a team comprised of the brightest minds in technology, research and mathematics. Our team includes PhD’s and Professors from the top commercial and academic institutions around the globe.<br /><br /><br />Requirements:<br />2+ years of relevant experience in Data Science<br />Proven experience in translating business challenges into data pipelines & model framework Customer facing experience and excellent presentation and communication skill<br />Able to initiate and drive projects to completion with minimal guidance<br />Experience with Machine Learning techniques (classification, regressions, feature selection etc.)<br />Programming skills that allow you to be self-sufficient in handling data (Python, SQL, Scala, Java)<br />Experience with large datasets and distributed computing (Hive/Hadoop, Spark)<br />Experience with statistical tools or packages (R / RapidMiner / Scikit)<br />MS/PhD in Math, Statistics, Computer Science, Physics, Bioinformatics or another quantitative field<br />SparkBeyond offers a highly competitive compensation package including complete benefits and the ability to work with a team comprised of the brightest minds in technology, research and mathematics. Our team includes PhD’s and Professors from the top institutions around the globe and therefore, we are quite selective in our hiring process.<br /><br />If you qualify and would like to join one of the fastest growing technology companies and have a real impact in helping to tackle and solve some of the world’s largest problems, please reach out to us.",
    "urlFetchPending": false,
    "published": "Mar 23, 2017 12:56:25 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "uzrJt2mvT8eIT4iU5_t-3Q",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55077/data-scientist-at-sweatcoin/",
    "title": "Data Scientist",
    "content": "Sweatcoin is looking to hire a Data Scientist based in the UK to further the development of our proprietary movement verification algorithm. We will cooperate with the Institute of Digital Health at the University of Warwick and InnovateUK and work with 15bn data points on 100,000+ users<br /><br />JOB DESRIPTION<br />Sweatcoin, a digital currency backed by physical movement, is currently looking for a Data Scientist to join our fast-growing team.<br /><br />Background<br />Users convert their steps into digital currency. This leads to the uplift in physical activity, general wellbeing and reduction in use of cars/public transport and carbon emissions. The Sweatcoins can be spent in an In-App Marketplace for offers from selected vendors or charities. More than 100 vendor partners accept Sweatcoins in the UK and the US. We have been live since May ’16 with more than 110,000 downloads and more than 12bn data points collected.<br />As for any currency, it is paramount for us to ensure that the Sweatcoin cannot be ‘faked’ and the movement converted to Sweatcoins is genuine. We have already developed a proprietary algorithm for outdoor movement verification. Now it’s the turn of indoor movement verification algorithm. To do so we have partnered with the Institute of Digital Healthcare at the University of Warwick and obtained a grant from InnovateUK. <br /><br />The Candidate<br />The successful candidate will be involved in setting up and running the data collection from the focus groups via a specially designed mobile app. S/he will be evaluating the data and finding patterns that will serve as benchmarks for the new movement verification algorithm. <br />This is a perfect opportunity for the successful candidate to become a part of an innovative and energetic team that delivers a unique (and working!) solution for sustainable behavior change.<br /><br />Responsibilities<br />•    Research and develop statistical learning models for data analysis <br />•    Collaborate with the development team to design the data collection app <br />•    Implement new statistical or other mathematical methodologies as needed for specific models or analyses <br />•    Optimize joint development efforts through using the corporate collaboration tools<br />•    Keep up-to-date with latest technology trends <br />•    Communicate results and ideas to the key decision makers <br /><br />Skills and Requirements<br />•    Master’s Degree in Computer Science, Statistics, Applied Math or related field <br />•    Practical experience with Octave, R, ETL, data processing, database programming and data analytics <br />•    Extensive background in data mining and statistical analysis <br />•    Able to understand various data structures and common methods in data transformation <br />•    Excellent pattern recognition and predictive modeling skills <br />•    Experience with programming languages such as ??? is an asset <br /><br />Personal Traits<br />•    Positive and reliable personality<br />•    Focus on self-development and continuous improvement<br />•    General erudition<br />•    Can do or knows something that the others don’t<br />•    Self-starter<br />•    Open-minded and creative<br />•    Passionate about fitness/sports<br />•    Fun and all-around nice person<br /><br />For more information on Sweatcoin see overleaf<br />",
    "urlFetchPending": false,
    "published": "Mar 7, 2017 5:09:53 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "ZPpPX9-LSomec0-c2rzGlQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-consultants",
    "url": "http://workinstartups.com/job-board/job/56148/data-analyst-at-4th-office/",
    "title": "Data Analyst",
    "content": "4th Office is a secure email app that upgrades your Gmail or Outlook inbox. It reorganizes your email around the people, tasks and teams that matter to you - so you can work smart, drive productivity, and supercharge collaboration.<br /><br />We understand that managing email can feel like hard work, but imagine a world where you love email... where email works hard for you, where you only have to focus on the good stuff, where you never miss important messages and where collaboration feels easier than ever. That's the world of 4th Office. <br /><br />•    Connect with people & avoid clutter <br />•    Get stuff done<br />•    Collaborate with teams <br /><br />With investments from serial entrepreneur Ben White and VC funds Notion and RSG Capital, we are on a mission to deliver an ultimate work hack built on top of your email or virtual online workplace will help people focus on what is important by helping them to work together and surfacing the important communication and tasks to one place. <br /><br />*Role / Job Purpose*        <br /><br />The successful candidate will analyse and turn data into meaningful insights that will make major influence on business decisions. The candidate should feel comfortable will wrangling vast amounts of data and presenting the results to the company leadership.<br /><br />Responsibilities <br /><br />*Work with management to prioritize business and information needs. Interpreting data, analyzing results using statistical techniques and providing ongoing reports<br />*Acquire data from primary or secondary data sources and maintain databases/data systems<br />*Work with Artificial Intelligence team and involve in the process of building “smart” features of the app<br />*Presenting results to company leadership and other stakeholders<br /><br />*Skills & Experience*<br /><br />*Solid knowledge of Python and/or R<br />*Familiarity with SQL and NoSQL databases<br />*Strong communication skills with emphasis on giving/receiving feedback<br />*Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy<br />*Strong presentation skills<br />*Ability to quickly adapt and tackle tasks depending on the current priority",
    "urlFetchPending": false,
    "published": "Apr 6, 2017 2:44:20 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "eUg8UgAoQY6OVKG9R3b0Ig",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14027350",
    "title": "Dataquest | Data Science/Data Engineering Instructor | San Francisco | Remote okay | $90k-$120k + equity",
    "content": "<div>                  <span>Dataquest | Data Science/Data Engineering Instructor | San Francisco | Remote okay | $90k-$120k + equity<p>At Dataquest (www.dataquest.io), we teach data science and data engineering to thousands of students around the world every day. We teach the concepts behind the code, then help students build projects until everything clicks. We get results -- we've had students get jobs at companies like SpaceX, and our NPS is around 60.</p><p>We don't believe in skimming the surface of concepts so that students just know what to type.  We build intuition around complicated ideas like random forests from the ground up.  We walk students through building their own algorithms, so we can help them understand the tradeoffs and limitations of techniques.  We help students build projects, so they can solidify their knowledge and get real-world experience.</p><p>We're looking for someone to help us extend and enhance our curriculum.  This involves writing instructional content, creating projects, thinking about how to improve how we teach, mentoring students directly, and getting feedback from students to improve our teaching methods.</p><p>We're looking for someone who's passionate about teaching, and shares our mission to give people access to high quality education at a low cost.  You'll have a lot of latitude to shape how we teach, and to help build a personalized educational platform. We're bootstrapped and profitable, so this is also a chance to learn more about the business side.</p><p>This is a great role if you want a make huge impact on the world, grow a business, and rapidly expand your skillset.</p><p>If this is interesting, please email vik at dataquest.io.<span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 4, 2017 12:31:05 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "_cGMgxiLRXaltm0avCIKIA",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14023203",
    "title": "Takt | Data Engineer | San Francisco, CA | Full-time, ONSITE preferred, but REMOTE is an option for senior candidates. Takt ...",
    "content": "<div>                  <span>Takt | Data Engineer | San Francisco, CA | Full-time, ONSITE preferred, but REMOTE is an option for senior candidates.Takt also has open positions for Systems and Infrastructure Engineer, Haskell Engineer, Data Scientist, Product Managers/Designers, and more. Check them out at <a href=&quot;http://takt.com/careers&quot; rel=&quot;nofollow&quot;>http://takt.com/careers</a>. Here is the Data Engineer job description:<p>Takt is seeking data engineers to help develop our flagship product. Our platform learns and adapts to people's preferences, habits, and feedback—orchestrating highly relevant experiences that are truly unique to each person. Our vision will change the way people engage across multiple industries, be it retail, finance, or healthcare.</p><p>We share your passion for using data to solve complex problems. You understand that legacy code is the work you did yesterday. You'll work in small, self-sufficient teams with a common goal: deliver excellent software anchored in an agile culture of quality, delivery, and innovation.  Contact mightybyte at the google mail service for more information.<span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 3, 2017 6:02:04 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "SCsocIAjS5GX5whz-A71gw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14024474",
    "title": "CyMetica | San Francisco | Remote OK | Interns Ok We build datasets for machine learning and AI efforts around the ...",
    "content": "<div>                  <span>CyMetica | San Francisco | Remote OK | Interns Ok<p>We build datasets for machine learning and AI efforts around the world. We are currently looking for anyone interested in creating triangulated data for datasets and feature engineering in the areas of Finance, Life Sciences and a few other areas e.g. <a href=&quot;http://54.174.116.134/recommend/datasets&quot; rel=&quot;nofollow&quot;>http://54.174.116.134/recommend/datasets</a> Algorithmic creation and testing is also needed from time to time.</p><p>Languages include: Python, js, and Tcl.</p><p>If interested in applying please contact cymetica@gmail.com<span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 3, 2017 7:53:04 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "4biWSxpiTbeSA-LKQ5dvog",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14045810",
    "title": "Pluralsight | Data Scientist | Salt Lake City, UT | Full-time",
    "content": "<div>                  <span>Pluralsight | Data Scientist | Salt Lake City, UT | Full-time<p>Do you like working with big data and using it to drive decisions and the direction of products that impact the lives of people every day? As a Data Scientist dedicated to decision support and strategy at Pluralsight, you will have the opportunity to see a direct link between your work and business critical questions, insights, and outcomes. You will be part of an established team of top-notch, collaborative data scientists working on breadth of challenges across the company and you will be working with a cutting-edge data platform. For senior members of the team, your responsibilities will also include providing leadership and mentoring to teammates and championing data-driven decision making throughout the organization</p><p>You can apply at: <a href=&quot;https://www.smartrecruiters.com/Pluralsight/106867668-data-scientist&quot; rel=&quot;nofollow&quot;>https://www.smartrecruiters.com/Pluralsight/106867668-data-s...</a>or email michelle-keim@pluralsight.com with questions.<span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 6, 2017 12:33:05 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "OXbp5AdmTIOrNABXPTf_2A",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14037410",
    "title": "NStack | London, UK | Full time | Onsite & Remote | Functional Programmers & Data Scientists | http://nstack.com NStack lets ...",
    "content": "<div>                  <span>NStack | London, UK | Full time | Onsite &amp; Remote | Functional Programmers &amp; Data Scientists | <a href=&quot;http://nstack.com&quot; rel=&quot;nofollow&quot;>http://nstack.com</a><p>NStack lets data analysts to do sophisticated data work in the cloud without a team of engineers. To accomplish this, we’re a building a platform for composable, data-driven microservices, using a mixture of Haskell and low-level Linux systems tech (including containers, systemd, IPC, with some typed DSLs, systems code, and distributed systems thrown in). Our aim is to use the fundamental lessons of programming languages and operating systems to provide an abstraction over infrastructure - think Bash for containerised microservices.</p><p>We’re looking for both talented programmers -- preferably with some knowledge of typed functional languages and *NIX systems programming -- and data scientists to join our team to make this a reality. It’s a challenging role, working on hard problems, and offers the chance to work with a top technical team and shape a company and product from an early stage.</p><p>NStack is funded by top-tier investors from the West Coast, the founders are both technical and ex-YC / academia, and our team is lucky enough to include world-class talent for the problem we're solving. Salaries are competitive and include generous stock options. EU applicants welcome for onsite, and remote is also possible for the right candidate. We’re looking at a range of positions and experience levels - whether you’ve just left uni or been hacking for 20 years, if you’re interested please get in touch.</p><p>Any questions please comment, reach out via jobs@nstack.com, or <a href=&quot;https://angellist.com/nstack/jobs&quot; rel=&quot;nofollow&quot;>https://angellist.com/nstack/jobs</a></p><p>Cheers!<span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 5, 2017 12:34:05 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "M5WLj8jRQ-mw7Mvm6PK0Xg",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14028452",
    "title": "Signifyd - Back End and Machine Learning Engineers - San Jose - Full Time",
    "content": "<div>                  <span>Signifyd - Back End and Machine Learning Engineers - San Jose - Full Time<p>www.signifyd.com/careers/</p><p>Our engineers build systems that catch bad guys.  Using all available payment, user, and machine data, we have to separate legitimate credit card transactions from fraudulent in under 400ms.  That means doing just-in-time mash-ups of internal data with external APIs and reducing it all into a single score with a few critical insights for end-users.</p><p>To solve this problem, we're looking for world-class engineers who are eager to learn, adopt, and contribute to a reactive style of programming.</p><p>Our stack:Java, Python, Cassandra, MySQL, Solr, Apache Spark, Play! framework, Linux, Docker, AWS</p><p><i>Signifyd was recently named In Forbes 50 Most Innovative Fintech Companies</i><span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 4, 2017 4:00:04 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_100",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "100% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "jeBe6lnfTTqas3ouD8CQNQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/141591/big-data-engineer-skytruth?a=Lu5dTws8eNG",
    "title": "Big Data Engineer at SkyTruth (Washington, DC) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Big Data Engineer</a>                            </h2>                        </div><a rel=&quot;nofollow&quot;>SkyTruth</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p>This is an extraordinary opportunity to get to<strong> use cutting-edge big data and machine learning tools&nbsp;</strong>while <strong>doing something good for the planet</strong> and <strong>open-sourcing</strong> all your code.</p><p>SkyTruth is seeking an engineer to join the team that is building<strong> <a href=&quot;http://globalfishingwatch.org/&quot; rel=&quot;nofollow&quot;>Global Fishing Watch</a></strong> which is a partnership of SkyTruth, Oceana and Google, <strong>supported by Leonardo DiCaprio</strong>, and dedicated to saving the world's oceans from ruinous overfishing <a href=&quot;http://www.wired.com/2014/11/plan-map-illegal-fishing-space/&quot; rel=&quot;nofollow&quot;>[Wired]</a>, &nbsp; Our team works directly with <strong>Google</strong> engineers that support <strong>Cloud ML, TensorFlow </strong>and<strong> DataFlow</strong> and we are a featured&nbsp;Google partner.</p><p><a href=&quot;https://cloud.google.com/customers/global-fishing-watch/&quot; rel=&quot;nofollow&quot;>https://cloud.google.com/customers/global-fishing-watch/</a></p><p><a href=&quot;https://environment.google/projects/fishing-watch/&quot; rel=&quot;nofollow&quot;>https://environment.google/projects/fishing-watch/</a></p><p><a href=&quot;https://blog.google/products/maps/mapping-global-fishing-activity-machine-learning/&quot; rel=&quot;nofollow&quot;>https://blog.google/products/maps/mapping-global-fishing-activity-machine-learning/</a></p><p>Your job&nbsp;is to develop, improve and operationalize the multiple pipelines we use to process terrabytes of vessel tracking data&nbsp;collected by a constellation of satellites. &nbsp;We have a data set containing billions of vessel position reports, from which we derive behaviors based on movement characteristics using Cloud ML, and publish a dynamically updated map of global commercial fishing activity.</p><p>You will join a fully distributed team of engineers, data scientists and designers who are building and <strong>open sourcing</strong> the&nbsp;next generation of the product&nbsp;and who are very committed to creating a positive impact in the world while also&nbsp;solving novel problems using&nbsp;cutting edge tools.&nbsp;</p><p>The company is headquartered&nbsp;in Washington DC, the data science team is in San Francisco, and we have engineers in the US, Europe, South America and Indonesia. &nbsp;Daily scrums are scheduled around east coast US timezone (so that kind of sucks for the guy in Indonesia :-)</p><p>Because&nbsp;this is open to remote work, we will get a lot of applicants. We are not just looking for an&nbsp;engineer&nbsp;with great skills that wants to work with cool tech. &nbsp;We also want you to be inspired by the project, so <strong>please tell us something that excites you about what we're doing</strong> when you contact us.&nbsp;</p><p>Here's some more stuff&nbsp;you can read about the impact our work has:</p><p><a title=&quot;New York Times: Palau vs the Poachers&quot; href=&quot;https://www.nytimes.com/2016/02/21/magazine/palau-vs-the-poachers.html&quot; rel=&quot;nofollow&quot;>New York Times: Palau vs the Poachers</a></p><p><a title=&quot;Science: Ending hide and seek at sea&quot; href=&quot;http://science.sciencemag.org/content/351/6278/1148&quot; rel=&quot;nofollow&quot;>Science: Ending hide and seek at sea</a></p><p><a title=&quot;Washington Post: How Google is helping to crack down on illegal fishing — from space&quot; href=&quot;https://www.washingtonpost.com/news/energy-environment/wp/2016/09/15/from-space-a-new-effort-to-crack-down-on-illegal-fishing-across-the-globe/?utm_term=.87f83119b7b8&quot; rel=&quot;nofollow&quot;>Washington Post: How Google is helping to crack down on illegal fishing — from space</a></p>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><p><strong>What You'll Do</strong></p><ul><li>Extend and improve existing data pipeline built using <strong>hadoop, luigi</strong> and <strong>bigquery</strong></li><li>Design and implement new data pipeline components using <strong>hadoop</strong>, <strong>cloud dataflow</strong>, <strong>tensorflow</strong></li><li>Architect and develop dynamic cluster scaling architecture with <strong>cloud dataproc</strong></li><li>Implement performance monitoring and metrics with <strong>stackdriver</strong>.</li><li>Implement uptime monitoring for the multiple pipelines</li><li>Collaborate with data science team to design and implement machine learning classification algorithms at scale.</li><li>Design and build tools for analysts to use to manually classify time-series data</li><li>Perform ad hoc statistical and data mining analyses</li><li>Publish internally developed tools as <strong>open source</strong> projects</li><li>Handle architectural and design considerations such as performance, scalability, reusability and flexibility issues</li><li>Review the technical design and perform code review of other developers’ work</li><li>Implement APIs for data access and data distribution</li><li>Help choose the right technology stack for the next generation data platform</li></ul><p><strong>What to Bring</strong></p><ul><li><strong>Python</strong> experience: 5+ years</li><li><strong>Scala</strong> experience: 2+ years</li><li>Experience with scalable data architecture – e.g. <strong>Hadoop</strong>, <strong>Cloud Dataflow</strong> etc</li><li>Experience working with big data techniques</li><li>Experience working with cloud platforms like Amazon, Azure or <strong>Google Cloud Platform</strong></li><li>Experience working with <strong>Docker</strong></li><li>Ability to work in a distributed team</li></ul><p><strong>Nice to Have</strong></p><ul><li>Experience with machine learning tools such as <strong>scikit-learn, caffe, tensorflow</strong></li><li><strong>Open source</strong> collaboration experience</li><li><strong>gdal</strong> and other spatial tools</li><li><strong>Java, R</strong></li><li><strong>Kubernetes</strong></li></ul>            </div>            <h3>                About SkyTruth            </h3>            <div><p>SkyTruth is a growing group of dedicated people using remote-sensing technologies to help protect the earth by making more of the impacts of human activity visible to everyone. We use scientifically credible satellite images and other visual technologies to create compelling pictures that vividly illustrate environmental impacts, and provide these pictures and supporting data to environmental advocates, policy-makers, the media, and the public. SkyTruth is a 501(c)(3) nonprofit organization headquartered in Shepherdstown, West Virginia (just outside Washington DC), with team members on three continents.</p>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 2:42:57 PM",
    "tags": [
      "TECH2_python",
      "TZ_America",
      "REMOTE1_50",
      "TECH2_tensorflow",
      "TECH2_hadoop",
      "TECH2_scala",
      "TECH2_google-cloud-dataflow",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": [
      "google-cloud-dataflow",
      "hadoop",
      "python",
      "scala",
      "tensorflow"
    ]
  },
  {
    "id": "E2K5ja3cQrW-lkvxNmCouQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/138291/front-end-developer-at-high-growth-social-media-brandbastion?a=Knt05E6QXNS",
    "title": "Front End Developer at High Growth Social Media Company (AI/machine learning) at BrandBastion (Helsinki, Finland) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Front End Developer at High Growth Social Media Company (AI/machine learning)</a>                            </h2>                                <span>                                    Equity                                </span>                        </div><a rel=&quot;nofollow&quot;>BrandBastion</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p><strong>DESCRIPTION</strong></p><p>BrandBastion is a rapidly growing company focused on disrupting the brand reputation and ad performance services on social media with the use of AI and machine learning. We work with Fortune 500 companies, governments, start-ups and non-profits in North America, South America, Europe, Asia and Australia. We are backed by great investors such as Mårten Mickos (HackerOne) and Risto Siilasmaa (founder of F-Secure).</p><p>We are scouting for a front end multi-talent to join our A-team full time:</p><p>You are passionate about designing, developing and implementing web interfaces and apps for next-gen services using frameworks such as React. You want to lead and drive change in your own area. You function best in a fast-paced environment with focus on execution. You are self-organized, self-motivated, a good communicator and you inspire others through your work.</p><p>You will:</p><ul><li>Lead the front end side of our software projects</li><li>Be actively involved in architecting new services and products together with our core team</li><li>Will work on building single-page real time web applications with focus on performance, usability and user experience</li><li>Be proactive and up-to-date with the latest changes in the industry</li><li>Share our values: focus on execution, adaptability and agility, knowledge above hierarchy, self-drive and constant improvement.</li></ul>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><ul><li>Experience with any of the following would be an asset: SaaS, CRM, real-time web applications, interactive dashboards</li><li>Extensive JavaScript knowledge (+3 years)</li><li>Working experience with React (+Redux), Angular (optional)</li><li>You like fast responsive interfaces and know how to optimize them</li><li>ES6, Node.js, NPM, CSS (SASS), D3.js</li><li>Data APIs, Websocket</li><li>Extensive testing and CI knowledge</li><li>UI/UX prototyping, usability, responsive design would be an asset</li><li>Good team communicator and collaborator with excellent English skills</li><li>Innovative and entrepreneurial mindset</li></ul>            </div>            <h3>                About BrandBastion            </h3>            <div><p>BrandBastion is a growth company specialized in real time protection of brand reputation and ad performance on social media. We fight trolls and pirates on a daily basis and remove harmful user-generated content from our clients' social media properties within 8 minutes 24/7.</p><p>We work with governments and Fortune 500 companies in for example gaming, entertainment, e-commerce and technology.</p><p>We've developed a large scale web platform that handles large amounts of real time social media data (Facebook, Instagram and YouTube, etc.) incorporated with innovative technologies such as machine learning and AI to bring a new standard to online brand protection.</p>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_reactjs",
      "TECH2_user-interface",
      "TZ_America",
      "REMOTE1_50",
      "TECH2_d3.js",
      "TECH2_javascript",
      "WORKAUTH_US",
      "TITLE1_data",
      "TECH2_single-page-application"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": [
      "d3.js",
      "javascript",
      "reactjs",
      "single-page-application",
      "user-interface"
    ]
  },
  {
    "id": "Jwt68VNOQqSktZ9LG8KA5w",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "stackoverflow.com",
    "url": "http://stackoverflow.com/jobs/132018/senior-data-science-engineer-adroll?a=Ih26Svrvems",
    "title": "Senior Data Science Engineer at AdRoll (San Francisco, CA) (allows remote)",
    "content": "<div>        <div>                <div>            <div>                                    <div>                    <div>                        <div>                            <h2><a rel=&quot;nofollow&quot;>Senior Data Science Engineer</a>                            </h2>                                <span>                                    Equity                                </span>                        </div><a rel=&quot;nofollow&quot;>AdRoll</a>                    </div>                </div>                <div>                    <div>                                                        </div>                </div>                <div></div>            </div>        </div>                        </div>        <div>            <h3>                Job Description            </h3>            <div><p><strong>About the Role:</strong></p><p>The Data Science Engineering team writes the math and software that sit at the heart of AdRoll’s real-time bidding technology, our core product. &nbsp;Our software runs over hundreds of terabytes of data daily to produce its models, which are then subsequently queried billions of times by hundreds of servers.</p><p>Current members of Data Science Engineering have a passion for not only mathematics, but also technology. If you join our team, you will have the opportunity to stay on the cutting edge of research and then to put this research to practical use by building your own software and infrastructure systems. When you push out your code, you will be able to see the results of your efforts ripple throughout our systems in real-time.<br><br>Moreover, AdRoll Engineering is broken out into small teams that have significant autonomy. We like to think of Data Science Engineering as a &quot;startup within AdRoll.&quot; You will design your systems as you see fit, own your code, and your work will have a very direct, measurable impact on AdRoll's core metrics.<br><br>Data Science Engineering is a small team with a variety of concurrent projects. Your interests dictate what projects you will tackle. The common thread among all projects is that they require a high level of mathematical sophistication and engineering prowess, due to the scale of our data.</p><p><strong>Current projects include:</strong></p><ul><li>Enhancing our machine learning software with the latest in machine learning algorithms</li><li>Employing machine learning and statistical modeling over our petabytes of data to generate new features to feed into our bid-time predictions</li><li>Creating systems that monitor inputs and performance on a real-time basis to inform engineers when unexpected changes arise.</li><li>Performing automated market analyses to dynamically react to changing market conditions</li></ul><p><strong>Responsibilities:</strong></p><ul><li>Discover new predictive insights in our dataset</li><li>Design systems that regularly compute new predictive models</li><li>Read and conduct research to develop mathematical solutions to our most pressing problems</li><li>Engineer to put your research into practice</li><li>Ownership of a system that is the core of our intelligence products</li></ul>            </div>        </div>            <h3>                Skills &amp; Requirements            </h3>            <div><p><strong>Qualifications:</strong></p><ul><li>BS or MS in Computer Science, Computer Engineering, Mathematics. Strength in both fields is necessary; double majors are a big plus.</li><li>3+ years experience with C, C++, or Java</li><li>Proven ability at architecting scalable, high performance systems</li><li>Experience translating math into efficient code</li><li>Enthusiasm to work in a fast-paced engineering team</li></ul><p><strong>Bonus Points:</strong></p><ul><li>Prior experience with machine learning algorithms and their implementations</li><li>Experience with manipulating large data sets</li><li>Understanding of online ad industry</li><li>Humor. Old school video games. Robots.</li></ul>            </div>            <h3>                About AdRoll            </h3>            <div><p><strong>Compensation:</strong></p><ul><li>Competitive salary and equity</li><li>Medical / Dental / Vision benefits</li><li>Paid time off and generous holiday schedule</li><li>The opportunity to win the coveted Golden Bagel award</li></ul><p><strong>About AdRoll:</strong></p><p>AdRoll is the largest independent retargeting and prospecting platform with over 25,000 clients worldwide. It specializes in performance marketing for several verticals including e-commerce, b2b, finance, travel, education, and media, and analyzes data on over 1 billion digital profiles across desktop, mobile, and tablet devices through its proprietary technology. Partnering with top media and tech companies including Google, Facebook, Twitter, Instagram, and Apple, along with millions of websites and mobile apps, the company is home to the world's largest opt-in advertiser data co-op. AdRoll’s goal is to map the world’s intent data and put it to work for every advertiser on the planet.</p><p>AdRoll is headquartered in San Francisco, with offices in New York, Tokyo, London, Dublin, and Sydney. It is backed by Foundation Capital, IVP, Accel Partners, Merus Capital, Peter Thiel, and other leading investors. Learn more at<a href=&quot;http://www.adroll.com/&quot; rel=&quot;nofollow&quot;> www.adroll.com</a>.</p><p><br>AdRoll is committed to creating diverse teams of “Rollers” and is proud to be an equal opportunity employer. All qualified applicants will receive consideration without regard to race, color, ancestry, sex, religion, gender, gender identity or expression, sexual orientation, marital status, national origin, citizenship, genetics, disability, age, veteran status or other characteristics.</p>            </div>                        </div>",
    "urlFetchPending": false,
    "published": "May 1, 2017 4:24:18 PM",
    "tags": [
      "TECH2_probability",
      "TECH2_c++",
      "TECH2_math",
      "TZ_America",
      "REMOTE1_50",
      "TECH2_java",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": [
      "c++",
      "java",
      "math",
      "probability"
    ]
  },
  {
    "id": "v87Z-YPJT7S_6FkgjhaxUg",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "remoteok.io",
    "url": "https://remoteok.io/jobs/23295",
    "title": "Big Data Engineer",
    "content": "<p>This is an extraordinary opportunity to get to<strong> use cutting-edge big data and machine learning tools&nbsp;</strong>while <strong>doing something good for the planet</strong> and <strong>open-sourcing</strong> all your code.</p><br><p>SkyTruth is seeking an engineer to join the team that is building<strong> <a href=&quot;http://globalfishingwatch.org/&quot; rel=&quot;nofollow&quot;>Global Fishing Watch</a></strong> which is a partnership of SkyTruth, Oceana and Google, <strong>supported by Leonardo DiCaprio</strong>, and dedicated to saving the world's oceans from ruinous overfishing <a href=&quot;http://www.wired.com/2014/11/plan-map-illegal-fishing-space/&quot; rel=&quot;nofollow&quot;>[Wired]</a>, &nbsp; Our team works directly with <strong>Google</strong> engineers that support <strong>Cloud ML, TensorFlow </strong>and<strong> DataFlow</strong> and we are a featured&nbsp;Google partner.</p><br><p><a href=&quot;https://cloud.google.com/customers/global-fishing-watch/&quot; rel=&quot;nofollow&quot;>https://cloud.google.com/customers/global-fishing-watch/</a></p><br><p><a href=&quot;https://environment.google/projects/fishing-watch/&quot; rel=&quot;nofollow&quot;>https://environment.google/projects/fishing-watch/</a></p><br><p><a href=&quot;https://blog.google/products/maps/mapping-global-fishing-activity-machine-learning/&quot; rel=&quot;nofollow&quot;>https://blog.google/products/maps/mapping-global-fishing-activity-machine-learning/</a></p><br><p>Your job&nbsp;is to develop, improve and operationalize the multiple pipelines we use to process terrabytes of vessel tracking data&nbsp;collected by a constellation of satellites. &nbsp;We have a data set containing billions of vessel position reports, from which we derive behaviors based on movement characteristics using Cloud ML, and publish a dynamically updated map of global commercial fishing activity.</p><br><p>You will join a fully distributed team of engineers, data scientists and designers who are building and <strong>open sourcing</strong> the&nbsp;next generation of the product&nbsp;and who are very committed to creating a positive impact in the world while also&nbsp;solving novel problems using&nbsp;cutting edge tools.&nbsp;</p><br><p>The company is headquartered&nbsp;in Washington DC, the data science team is in San Francisco, and we have engineers in the US, Europe, South America and Indonesia. &nbsp;Daily scrums are scheduled around east coast US timezone (so that kind of sucks for the guy in Indonesia :-)</p><br><p>Because&nbsp;this is open to remote work, we will get a lot of applicants. We are not just looking for an&nbsp;engineer&nbsp;with great skills that wants to work with cool tech. &nbsp;We also want you to be inspired by the project, so <strong>please tell us something that excites you about what we're doing</strong> when you contact us.&nbsp;</p><br><p>Here's some more stuff&nbsp;you can read about the impact our work has:</p><br><p><a title=&quot;New York Times: Palau vs the Poachers&quot; href=&quot;https://www.nytimes.com/2016/02/21/magazine/palau-vs-the-poachers.html&quot; rel=&quot;nofollow&quot;>New York Times: Palau vs the Poachers</a></p><br><p><a title=&quot;Science: Ending hide and seek at sea&quot; href=&quot;http://science.sciencemag.org/content/351/6278/1148&quot; rel=&quot;nofollow&quot;>Science: Ending hide and seek at sea</a></p><br><p><a title=&quot;Washington Post: How Google is helping to crack down on illegal fishing — from space&quot; href=&quot;https://www.washingtonpost.com/news/energy-environment/wp/2016/09/15/from-space-a-new-effort-to-crack-down-on-illegal-fishing-across-the-globe/?utm_term=.87f83119b7b8&quot; rel=&quot;nofollow&quot;>Washington Post: How Google is helping to crack down on illegal fishing — from space</a></p>",
    "urlFetchPending": false,
    "published": "Apr 29, 2017 12:33:00 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_50",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "C6K-nPYIRyWiMq2TlO08wA",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55625/data-engineer-mf-at-project-a/",
    "title": "Data Engineer (m/f)",
    "content": "Project A is an operational VC that provides its ventures with capital, an extensive network and exclusive access to a wide range of operational expertise. The Berlin-based investor makes use of the €220m in assets under its management to back early-stage companies in the digital technology space. With its unique organisational structure featuring 100 operational experts, Project A offers its portfolio companies hands-on support in the areas of IT, Marketing & Brand Building, Business Intelligence, Sales and Recruiting. The portfolio includes companies such as Catawiki, WorldRemit, Tictail, Contorion, nu3, Lostmy.name and ZenMate.<br /><br />For one of our portfolio companies we are looking to fill the following position as soon as possible<br /><br />Data Engineer (m/f)<br /><br /><br />Your Tasks:<br /><br />•    You will help our business intelligence team to build data driven applications for our ventures: data warehouses, recommendation engines and CRM systems (developed in-house, based on open-source technologies). See https://www.youtube.com/watch?v=EVMvC9Ov6Vw for an overview of how we do this<br />•    You will integrate data from various systems into flexible and consistent representations. You will make sure that all people and IT systems in the organisation have an easy access to the data through various front-ends and interfaces<br />•    You will advance our software architecture and tool set to growing challenges and data amounts (performance, scaling, data quality)<br />•    You will work in an agile software development process in close collaboration with a product management team<br /><br />Your Profile:<br /><br />•    You have at least a master's degree in computer science or a comparable qualification<br />•    You have a genuine interest in data and algorithms and you are excited about solving difficult problems. You strive for efficient and robust solutions<br />•    You master at least these basic tools of computer science: object oriented programming in multiple languages, HTTP and current web technologies, the unix command line and basic server administration, version control systems, a basic understanding of the interplay between software and memory, hard discs and the CPU<br />•    You have profound knowledge about the inner workings of database systems.<br />•    You are eager to delve into new technologies and programming languages (our current stack: Mac or Linux, PostgreSQL, Mondrian & MDX, Python, Java, Php, ElasticSearch)<br />•    You have a basic understanding of mathematics and machine learning<br /><br />Our Benefits:<br /><br />•    You will join a highly professional and motivated team<br />•    You will have the unique opportunity to witness the launch of a newly established company and you can contribute your own ideas to its development<br />•    You will benefit from the greatest possible creative freedom to develop your skills further<br />•    You will enjoy a state-of-the-art, top-equipped workplace right in the center of Berlin<br />•    You will benefit from a communicative, stimulating and inspiring environment<br /><br />Do you want to become part of our success story?<br />We are looking forward to you online application.<br /><br />",
    "urlFetchPending": false,
    "published": "Mar 22, 2017 3:27:19 PM",
    "tags": [
      "TZ_America",
      "REMOTE1_50",
      "TITLE1_data",
      "WORKAUTH_EU"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "EU work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "v3oYoaJLRny-_vXeMO0ccw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "workinstartups-programmers",
    "url": "http://workinstartups.com/job-board/job/55600/junior-data-engineer-at-erevalue-ltd/",
    "title": "Junior Data Engineer",
    "content": "All about us:<br />                    <br />We are eRevalue, a new technology (SaaS) company based in 5 countries. We’re headquartered in London, but also have offices in the US, Spain, India, and Luxembourg. Our mission is to positively impact the world by helping companies create sustainable value. We do this through pioneering technology, subject expertise, and a lot of passion!<br />We’re a blend of different professional backgrounds and personalities: 17 nationalities with experience in technology, law, finance, ESG, and design. This mix makes eRevalue a unique and interesting workplace, and a close-knit one. We often say that eRevalue is more like a family than a business, and it’s a formula that has worked for us so far!<br /><br />What we are looking for:<br />                    <br />We’re looking for a Junior Data Engineer to join our fast-growing team. You will join our Technology team and help build our software program further. That said, we’d like you to join us on the big waves of innovation. <br /><br />When? Starting date: as soon as possible<br />Where? Location: London, UK<br />Duration? Permanent<br /><br />You will:<br /> <br />Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries).<br />Work closely with our data scientist team to integrate your innovations and algorithms into production systems.<br />Process unstructured data into a form suitable for analysis – and then do the analysis.<br />Support business decisions with ad hoc analysis as needed.<br />Use Python, SQL and NoSQL db on AWS infrastructure<br /><br /><br />Relevant experience:<br /><br />We’re all about diversity, but the following is essential:<br />Bachelors or Masters degree (or equivalent) in computer Science<br />Programming experience in Python <br />Experience working with SQL or NoSQL db (Mongodb)<br /><br /><br />Desirable if you have:<br /><br />Experience with public cloud preferably AWS(EC2, S3)<br />Experience with ElasticSearch, MapReduce, Spark<br />Experience processing large amounts of structured and unstructured data<br />Worked with software development testing tools<br /><br /><br />Benefits:<br /><br />And in return we offer:<br /><br />A dynamic and international work environment based in the heart of London’s vibrant tech area<br />25 days of holidays excluding bank holidays<br />Annual stock options<br />Free gym membership<br />Free drinks and snacks -  you can help yourself<br />Relaxed and flexible work culture<br />Frequent team drinks <br />Weekly team lunch<br />",
    "urlFetchPending": false,
    "published": "Mar 22, 2017 11:49:21 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_50",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "V5Lwd7g9SvahOrsQRdSqSw",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14023675",
    "title": "Factual | Engineers and data lovers | Los Angeles, San Francisco, Shanghai | www.factual.com/jobs#openings",
    "content": "<div>                  <span>Factual | Engineers and data lovers | Los Angeles, San Francisco, Shanghai | www.factual.com/jobs#openings<p>Factual is currently hiring engineers and data lovers of all levels in the SF Bay Area, Los Angeles, and Shanghai.</p><p>Factual’s location platform enriches mobile location signals with definitive global data, enabling personalized and contextually relevant mobile experiences. Built from billions of inputs, the data is constantly updated by Factual’s real-time data stack. We were named one of &quot;50 Disruptive Companies in 2013&quot; by MIT Technology Review. We have a terrific team that is still fairly small and an incredible CEO who was previously the co-founder of Applied Semantics (which was bought by Google and became AdSense). Factual has venture funding from Andreessen-Horowitz and our partners/customers include Bing, Apple, Facebook and Groupon.</p><p>There are many challenging problems to work on at all layers of the stack: data cleaning and canonicalization, storage, deduping, serving, APIs, improving data using machine learning, etc. A great example is one of our most recent products, Geopulse Audience, which stands at the intersection of high quality places data and large scale analysis of user geo-data: <a href=&quot;http://www.factual.com/products/geopulse-audience&quot; rel=&quot;nofollow&quot;>http://www.factual.com/products/geopulse-audience</a> . If you love data, Factual is the place to be. Our main criteria are that you're smart and get things done, but you'll get bonus points for experience with Clojure (<a href=&quot;http://www.factual.com/jobs/clojure&quot; rel=&quot;nofollow&quot;>http://www.factual.com/jobs/clojure</a>), machine learning, NLP, algorithm design, or Hadoop/Spark.</p><p>You can email me personally at alexr@factual.com, or view our job postings here: <a href=&quot;https://www.factual.com/jobs#openings&quot; rel=&quot;nofollow&quot;>https://www.factual.com/jobs#openings</a><span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 3, 2017 6:48:04 PM",
    "tags": [
      "REMOTE1_50",
      "TZ_America_West",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "50% remote",
      "America West timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  },
  {
    "id": "AtPkbxc2SG2xYTsjW6Z2LQ",
    "source": {
      "contentTransform": "TEXT_STRIP_FORMATTING",
      "jobAdContentRegExp": ""
    },
    "sourceName": "hackernews",
    "url": "https://news.ycombinator.com/item?id=14027192",
    "title": "7scientists | Co-Founder, Backend Developer, Frontend Developer, Data Scientist | Berlin | full-time or part time | on-site or remote",
    "content": "<div>                  <span>7scientists | Co-Founder, Backend Developer, Frontend Developer, Data Scientist | Berlin | full-time or part time | on-site or remote <a href=&quot;https://7scientists.com&quot; rel=&quot;nofollow&quot;>https://7scientists.com</a><p>We develop solutions for secure and scalable data analysis in hybrid environments (cloud + on-premise / own infrastructure). We build transparent, accountable and fair algorithms for machine learning and AI, and make sure companies can process sensitive data in a secure and reliable way.</p><p>Our office is located in a newly refurbished startup center in West Berlin (Charlottenburg), but we're also open to remote work if that's what you're into. We have only a very limited amount of money but are looking for at least one more founder to join the team (we're two currently), so if you are passionate about data analysis, privacy and IT security and want to work with an experienced and well-networked team, let's talk. We are currently fully self-funded, if you want to help us on our journey and become part of the team please get in touch with me: andreas@7scientists.com</p><p><a href=&quot;https://7scientists.com/en&quot; rel=&quot;nofollow&quot;>https://7scientists.com/en</a><span>              </span></p><div>        <p>                        </p></div></span></div>",
    "urlFetchPending": false,
    "published": "Apr 4, 2017 12:15:05 AM",
    "tags": [
      "TZ_America",
      "REMOTE1_50",
      "WORKAUTH_US",
      "TITLE1_data"
    ],
    "tagsNames1": [
      "50% remote",
      "America timezones",
      "US work authorisation"
    ],
    "tagsNames2": []
  }
]
}
